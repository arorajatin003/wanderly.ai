{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f50947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import string\n",
    "import contractions\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from googleapiclient.discovery import build\n",
    "import time\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_mapmyindia_token(client_id, client_secret):\n",
    "    url = \"https://outpost.mapmyindia.com/api/security/oauth/token\"\n",
    "    payload = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret\n",
    "    }\n",
    "    response = requests.post(url, data=payload)\n",
    "    data = response.json()\n",
    "    return data[\"access_token\"]\n",
    "access_token = get_mapmyindia_token(ClientId,Client_Secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a80063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_mapmyindia(place_name, token):\n",
    "    search_url = f\"https://atlas.mapmyindia.com/api/places/geocode\"\n",
    "    params = {\"address\": place_name}\n",
    "    # params = {\"\": place_name}\n",
    "    headers = {\"Authorization\": f\"bearer {token}\"}\n",
    "\n",
    "    response = requests.get(search_url, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "    return data\n",
    "    if data.get(\"suggestedLocations\"):\n",
    "        loc = data[\"suggestedLocations\"][0]\n",
    "        return float(loc[\"latitude\"]), float(loc[\"longitude\"])\n",
    "    \n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5f7de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lon_from_eloc(eloc, token):\n",
    "    url = f\"https://atlas.mapmyindia.com/api/places/geocode?eloc={eloc}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"bearer {token}\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "    return data\n",
    "    if \"copResults\" in data:\n",
    "        lat = data[\"copResults\"][\"latitude\"]\n",
    "        lon = data[\"copResults\"][\"longitude\"]\n",
    "        return float(lat), float(lon)\n",
    "    \n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b0cd8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21588788-c221-4056-8c17-7354bf30a24f'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f34af82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copResults': {'houseNumber': '',\n",
       "  'houseName': '',\n",
       "  'poi': 'Mubarak Mahal',\n",
       "  'street': '',\n",
       "  'subSubLocality': '',\n",
       "  'subLocality': 'Janta Bazaar',\n",
       "  'locality': 'Kanwar Nagar',\n",
       "  'village': '',\n",
       "  'subDistrict': 'Jaipur',\n",
       "  'district': 'Jaipur District',\n",
       "  'city': 'Jaipur',\n",
       "  'state': 'Rajasthan',\n",
       "  'pincode': '302002',\n",
       "  'formattedAddress': 'Mubarak Mahal, Janta Bazaar, Kanwar Nagar, Jaipur, Jaipur District, Jaipur, Rajasthan, 302002',\n",
       "  'eLoc': '8B2A6T',\n",
       "  'geocodeLevel': 'poi',\n",
       "  'confidenceScore': 0.4}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_coordinates_mapmyindia('mubarak mahal, Jaipur, Rajasthan',access_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37d4221b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'Missing address parameter'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lat_lon_from_eloc('8B2A6T',access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d319bb1",
   "metadata": {},
   "source": [
    "### Key word Extractring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ab507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_statistics(video_id): \n",
    "    # video_id = '7cPLbiblb84'\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "        request = youtube.videos().list(\n",
    "            part=['snippet','statistics'],\n",
    "            id=video_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "        stats = response['items'][0]['statistics']\n",
    "        viewCount = stats['viewCount'] if 'viewCount' in stats.keys() else 0\n",
    "        likeCount = stats['likeCount'] if 'likeCount' in stats.keys() else 0\n",
    "        commentCount = stats['commentCount'] if 'commentCount' in stats.keys() else 0 \n",
    "        channelId = response['items'][0]['snippet']['channelId']\n",
    "        channel_name = response['items'][0]['snippet']['channelTitle']\n",
    "\n",
    "        print(response['items'][0]['statistics'],channelId,channel_name)\n",
    "        \n",
    "        return int(viewCount),int(likeCount),int(commentCount),channel_name,channelId\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def get_channel_statistics(channelId):\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "        request_channel =  youtube.channels().list(\n",
    "            part=['statistics'],\n",
    "            id=channelId\n",
    "        )\n",
    "        response_channel = request_channel.execute()\n",
    "        subscriberCount= response_channel['items'][0]['statistics']['subscriberCount']\n",
    "        return int(subscriberCount)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def cleaning_sentence(text):\n",
    "    text = text.lower()\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    ans = contractions.fix(text).translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "    ans = ans.replace('music','')\n",
    "    ans = ans.replace(\"  \",' ')\n",
    "    ans = \" \".join(ans.split())\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4b95624",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video_ids = [\n",
    "    'p0MvovsCxCk',\n",
    "    '7cPLbiblb84',\n",
    "    'rm_j6O8y148',\n",
    "    'Lv0PkSkKeSo',\n",
    "    '5zA6OFpkPe0',\n",
    "    'Oz18u64bM8I',\n",
    "    'p0MvovsCxCk',\n",
    "    '7cPLbiblb84',\n",
    "    'Wt1LgJyF3s',\n",
    "    'fP2zols1dag',\n",
    "    'D1blpY-3ROE',\n",
    "    'snwAYESRUEw',\n",
    "    '48TcOu9kPqg',\n",
    "    'dcRh1zTPnDQ',\n",
    "    'lmhzpFfuWKc',\n",
    "    '45qqtIpQP5M',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7663e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0MvovsCxCk\n",
      "{'viewCount': '281734', 'favoriteCount': '0', 'commentCount': '97'} UCoCU-RvDqV1JgmbRgh79UTQ Singhsman \n",
      "7cPLbiblb84\n",
      "{'viewCount': '2033200', 'likeCount': '16201', 'favoriteCount': '0', 'commentCount': '864'} UC_P_sA6Jf3iSsFueMwIP3vg TheSocialTraveller\n",
      "rm_j6O8y148\n",
      "{'viewCount': '192311', 'likeCount': '2535', 'favoriteCount': '0', 'commentCount': '93'} UCQ0X2x6lozB7qG30ctFeUiA Saturday Shooters\n",
      "Lv0PkSkKeSo\n",
      "{'viewCount': '117108', 'likeCount': '2218', 'favoriteCount': '0', 'commentCount': '188'} UC2N5r2FvEOiDPKvaLW9rivw Aniruddha Patil\n",
      "5zA6OFpkPe0\n",
      "{'viewCount': '10204', 'likeCount': '201', 'favoriteCount': '0', 'commentCount': '33'} UCCq9CLWwVP4fdYy2N_ypTjg Sisters vs Globe\n",
      "Oz18u64bM8I\n",
      "{'viewCount': '9933', 'likeCount': '170', 'favoriteCount': '0', 'commentCount': '36'} UC-w8ULGFYLrjbdsZ6twWO3Q Travel Tales\n",
      "p0MvovsCxCk\n",
      "{'viewCount': '281734', 'favoriteCount': '0', 'commentCount': '97'} UCoCU-RvDqV1JgmbRgh79UTQ Singhsman \n",
      "7cPLbiblb84\n",
      "{'viewCount': '2033200', 'likeCount': '16201', 'favoriteCount': '0', 'commentCount': '864'} UC_P_sA6Jf3iSsFueMwIP3vg TheSocialTraveller\n",
      "Wt1LgJyF3s\n",
      "list index out of range\n",
      "cannot unpack non-iterable NoneType object\n",
      "fP2zols1dag\n",
      "{'viewCount': '357696', 'likeCount': '5125', 'favoriteCount': '0', 'commentCount': '194'} UCkzxUuB96SsG5LyC-SnpELA SugarSpiceNice Hindi\n",
      "D1blpY-3ROE\n",
      "{'viewCount': '460459', 'likeCount': '4956', 'favoriteCount': '0', 'commentCount': '212'} UCOsYbKWJfxGlpoYgLIJToeg SugarSpiceNice India\n",
      "snwAYESRUEw\n",
      "{'viewCount': '605216', 'likeCount': '16984', 'favoriteCount': '0', 'commentCount': '469'} UCnwL537dF3kV8hGVHvvof3Q Golgappa Girl\n",
      "48TcOu9kPqg\n",
      "{'viewCount': '933070', 'likeCount': '8595', 'favoriteCount': '0', 'commentCount': '230'} UC6tyFndkCnlFQ-VcpQauPVQ Traveller Rishabh\n",
      "dcRh1zTPnDQ\n",
      "{'viewCount': '373730', 'likeCount': '5866', 'favoriteCount': '0', 'commentCount': '28'} UClpdBL-6HiuWx2bgl1idViw Yatra Mitra\n",
      "lmhzpFfuWKc\n",
      "{'viewCount': '109348', 'likeCount': '1560', 'favoriteCount': '0', 'commentCount': '52'} UCG8kbqr_aiMJWbEZVcHzvIw Sakshi Yadav\n",
      "45qqtIpQP5M\n",
      "{'viewCount': '123274', 'likeCount': '1246', 'favoriteCount': '0', 'commentCount': '112'} UCY4rE2X-n2-TM_4K65CfXew Siddhartha Joshi\n",
      "[Errno 2] No such file or directory: 'd:\\\\randomProjects\\\\wanderly.ai\\\\data\\\\transcripts/transcript_45qqtIpQP5M.txt'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma_word         word suggested_by     video_id  key_word_extracted\n",
       "0   sindhi dal   sindhi dal   Singhsman   p0MvovsCxCk                   0\n",
       "1  kudi chhola  kudi chhola   Singhsman   p0MvovsCxCk                   0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "model_path = os.path.join(root.split('notebooks')[0], 'models', 'ner_mod','model-best')\n",
    "nlp_ner = spacy.load(model_path)\n",
    "nlp     = spacy.load(\"en_core_web_lg\")\n",
    "ner     = dict()\n",
    "    \n",
    "for video_id in test_video_ids:\n",
    "    try:\n",
    "        print(video_id)\n",
    "        \n",
    "        viewCount,likeCount,commentCount,channel_name,channelId = get_video_statistics(video_id)\n",
    "        subscriberCount = get_channel_statistics(channelId)\n",
    "\n",
    "        root = os.getcwd()\n",
    "        file_path = os.path.join(root.split('notebooks')[0], 'data', 'transcripts')\n",
    "\n",
    "        file = open(f'{file_path}/transcript_{video_id}.txt',\"r\")\n",
    "        file_c = open(f'{file_path}/transcript_{video_id}.txt',\"r\")\n",
    "        file_string = file_c.read()\n",
    "        for text in file:\n",
    "            doc_ner = nlp_ner(cleaning_sentence(text)) \n",
    "\n",
    "            for ent in doc_ner.ents:\n",
    "                # print(ent,'->', ent.label_)\n",
    "                \n",
    "                doc = nlp(ent.text)\n",
    "\n",
    "                rating = file_string.count(ent.text) + ((viewCount+likeCount+commentCount+subscriberCount)/(subscriberCount))\n",
    "        \n",
    "                lemma = ' '.join([token.lemma_ for token in doc])\n",
    "                if lemma in ner.keys():\n",
    "                    # ner[ent.label_].append(ent.text)\n",
    "                    ner[lemma]['ENT'].add(ent.label_)\n",
    "                    ner[lemma]['rating'] = (rating+ner[lemma]['rating'])/2\n",
    "                    ner[lemma]['suggested_by'].add(channel_name)\n",
    "                    ner[lemma]['video_id'].add(video_id)\n",
    "                else:\n",
    "                    # ner[ent.label_] = [ent.text]\n",
    "                    ner[lemma] = {\n",
    "                        'word': ent.text,\n",
    "                        'ENT': set([ent.label_]),\n",
    "                        # 'keywords': list,\n",
    "                        'suggested_by': set([channel_name]),\n",
    "                        'rating': rating,\n",
    "                        'video_id':set([video_id])\n",
    "                    }\n",
    "            # print('--x-x-x--x-x-x-x-x-x-x---x-x-x--')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# tokkenization()\n",
    "df = pd.DataFrame(ner)\n",
    "df = df.T\n",
    "df = df.reset_index().rename(columns={'index':'lemma_word'})\n",
    "\n",
    "df['ENT_len'] = df['ENT'].apply(lambda x: len(list(x))) \n",
    "df['ENT_max'] = df['ENT'].apply(lambda x: (list(x))[0])\n",
    "\n",
    "df[df['lemma_word']!=df['word']]\n",
    "df.to_csv('D:/randomProjects/wanderly.ai/data/Parquet/all_locations_&_food.csv')\n",
    "df_base_info = df[['lemma_word','word','suggested_by','video_id']]\n",
    "df_entity_info = df[['lemma_word','rating','ENT_max']]\n",
    "\n",
    "df_base_info = df_base_info.explode(['suggested_by','video_id']).drop_duplicates().reset_index()\n",
    "del df_base_info['index']\n",
    "df_base_info['key_word_extracted'] = 0\n",
    "\n",
    "df_base_info.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9b02002d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>{'Singhsman '}</td>\n",
       "      <td>{'p0MvovsCxCk'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>{'Singhsman '}</td>\n",
       "      <td>{'p0MvovsCxCk'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma_word         word    suggested_by         video_id  \\\n",
       "0   sindhi dal   sindhi dal  {'Singhsman '}  {'p0MvovsCxCk'}   \n",
       "1  kudi chhola  kudi chhola  {'Singhsman '}  {'p0MvovsCxCk'}   \n",
       "\n",
       "   key_word_extracted  \n",
       "0                   0  \n",
       "1                   0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_base_info = df[['lemma_word','word','suggested_by','video_id']]\n",
    "# df_entity_info = df[['lemma_word','rating','ENT_max']]\n",
    "\n",
    "# df_base_info = df_base_info.explode(['suggested_by','video_id']).drop_duplicates().reset_index()\n",
    "# del df_base_info['index']\n",
    "# df_base_info['key_word_extracted'] = 0\n",
    "# df_base_info.head(2)\n",
    "df = pd.read_csv('D:/randomProjects/wanderly.ai/data/Parquet/all_locations_&_food.csv')\n",
    "df_base_info = df[['lemma_word','word','suggested_by','video_id']]\n",
    "df_entity_info = df[['lemma_word','rating','ENT_max']]\n",
    "\n",
    "df_base_info = df_base_info.explode(['suggested_by','video_id']).drop_duplicates().reset_index()\n",
    "del df_base_info['index']\n",
    "df_base_info['key_word_extracted'] = 0\n",
    "\n",
    "df_base_info.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e628f810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rice</td>\n",
       "      <td>rice</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hai pri</td>\n",
       "      <td>hai pri</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tamarind sauce</td>\n",
       "      <td>tamarind sauce</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>lane tripolia</td>\n",
       "      <td>lane tripolia</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>agar</td>\n",
       "      <td>agar</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>pardesh manya</td>\n",
       "      <td>pardesh manya</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>bapu</td>\n",
       "      <td>bapu</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>sab jaanu</td>\n",
       "      <td>sab jaanu</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>482 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lemma_word            word  suggested_by     video_id  \\\n",
       "0        sindhi dal      sindhi dal    Singhsman   p0MvovsCxCk   \n",
       "1       kudi chhola     kudi chhola    Singhsman   p0MvovsCxCk   \n",
       "2              rice            rice    Singhsman   p0MvovsCxCk   \n",
       "3           hai pri         hai pri    Singhsman   p0MvovsCxCk   \n",
       "4    tamarind sauce  tamarind sauce    Singhsman   p0MvovsCxCk   \n",
       "..              ...             ...           ...          ...   \n",
       "477   lane tripolia   lane tripolia  Sakshi Yadav  lmhzpFfuWKc   \n",
       "478            agar            agar  Sakshi Yadav  lmhzpFfuWKc   \n",
       "479   pardesh manya   pardesh manya  Sakshi Yadav  lmhzpFfuWKc   \n",
       "480            bapu            bapu  Sakshi Yadav  lmhzpFfuWKc   \n",
       "481       sab jaanu       sab jaanu  Sakshi Yadav  lmhzpFfuWKc   \n",
       "\n",
       "     key_word_extracted  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "..                  ...  \n",
       "477                   0  \n",
       "478                   0  \n",
       "479                   0  \n",
       "480                   0  \n",
       "481                   0  \n",
       "\n",
       "[482 rows x 5 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_base_info.explode(['suggested_by','video_id'])\n",
    "df_base_info['video_id'] = df_base_info[df_base_info['video_id'].notna()]['video_id'].apply(lambda x :(x.replace(\"{\",'').replace(\"}\",'').replace(\"'\",'').split(\", \")))\n",
    "df_base_info['suggested_by'] = df_base_info[df_base_info['suggested_by'].notna()]['suggested_by'].apply(lambda x :(x.replace(\"{\",'').replace(\"}\",'').replace(\"'\",'').split(\", \")))\n",
    "\n",
    "df_base_info = df_base_info.explode(['suggested_by','video_id']).drop_duplicates().reset_index()\n",
    "del df_base_info['index']\n",
    "df_base_info['key_word_extracted'] = 0\n",
    "df_base_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9fdc644b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((482, 5), (388, 9))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info.shape,df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "21e5f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_window(id, text, window_size=50):\n",
    "    root = os.getcwd().split('wanderly.ai')[0]\n",
    "    file_path = os.path.join(root, f'wanderly.ai/data/transcripts/')\n",
    "    file = open(file_path + f'transcript_{id}.txt',\"r\")\n",
    "    \n",
    "    file_string = file.read()\n",
    "    print(file_path + f'transcript_{id}.txt')\n",
    "    # loc = 'city palace'\n",
    "    start = file_string.find(text)\n",
    "    end = file_string.rfind(text)\n",
    "    context_text = (file_string[start-window_size:end+window_size])\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f563531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Oz18u64bM8I.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Oz18u64bM8I.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Oz18u64bM8I.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Oz18u64bM8I.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_fP2zols1dag.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_D1blpY-3ROE.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_snwAYESRUEw.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_48TcOu9kPqg.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_dcRh1zTPnDQ.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_lmhzpFfuWKc.txt\n"
     ]
    }
   ],
   "source": [
    "df_base_info['details'] = df_base_info.apply(lambda x: get_context_window(x['video_id'],x['word'],100),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Removes stop words and lemmatizes the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text with stop words removed and lemmatized.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    filtered_and_lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and len(token.text)>2]\n",
    "    index = 1\n",
    "    while index<len(filtered_and_lemmatized_tokens):\n",
    "        if filtered_and_lemmatized_tokens[index] == filtered_and_lemmatized_tokens[index-1]:\n",
    "            del filtered_and_lemmatized_tokens[index-1]\n",
    "        else:\n",
    "            index = index+1\n",
    "\n",
    "    return \" \".join(filtered_and_lemmatized_tokens).replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "48e04b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>n have come well in the morning time very much...</td>\n",
       "      <td>come morning time breakfast center breakfast g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>st of our sindhis you have heard that dal dish...</td>\n",
       "      <td>sindhis hear dal dish sindhis people feel good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rice</td>\n",
       "      <td>rice</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>at dal dish in sindhis people feel very good a...</td>\n",
       "      <td>dal dish sindhis people feel good absolutely k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hai pri</td>\n",
       "      <td>hai pri</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>i rice is ok these three or four things are ma...</td>\n",
       "      <td>rice thing maintain morning pricing hai pri si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tamarind sauce</td>\n",
       "      <td>tamarind sauce</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>add ok so the lentils go but you have a little...</td>\n",
       "      <td>add lentil little bit chickpea onion tamarind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>lane tripolia</td>\n",
       "      <td>lane tripolia</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "      <td>you will find hawa mahal and in the hand side ...</td>\n",
       "      <td>find hawa mahal hand left jori bazaar call lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>agar</td>\n",
       "      <td>agar</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "      <td>d there i did not do the bark there how cannot...</td>\n",
       "      <td>bark look good mirror good lagar hai 450 last ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>pardesh manya</td>\n",
       "      <td>pardesh manya</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "      <td>o now we have come to jaipurs main work get\\na...</td>\n",
       "      <td>come jaipurs main work miss turban tie come de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>bapu</td>\n",
       "      <td>bapu</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "      <td>age\\nwell\\nbecause there is no many markets h...</td>\n",
       "      <td>age market link confusing bapu bazaar jauri ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>sab jaanu</td>\n",
       "      <td>sab jaanu</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "      <td>what new has come\\nho lak hai jaa ja jaane dat...</td>\n",
       "      <td>new come lak hai jaa jaane dat krav hey pity z...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>482 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lemma_word            word  suggested_by     video_id  \\\n",
       "0        sindhi dal      sindhi dal    Singhsman   p0MvovsCxCk   \n",
       "1       kudi chhola     kudi chhola    Singhsman   p0MvovsCxCk   \n",
       "2              rice            rice    Singhsman   p0MvovsCxCk   \n",
       "3           hai pri         hai pri    Singhsman   p0MvovsCxCk   \n",
       "4    tamarind sauce  tamarind sauce    Singhsman   p0MvovsCxCk   \n",
       "..              ...             ...           ...          ...   \n",
       "477   lane tripolia   lane tripolia  Sakshi Yadav  lmhzpFfuWKc   \n",
       "478            agar            agar  Sakshi Yadav  lmhzpFfuWKc   \n",
       "479   pardesh manya   pardesh manya  Sakshi Yadav  lmhzpFfuWKc   \n",
       "480            bapu            bapu  Sakshi Yadav  lmhzpFfuWKc   \n",
       "481       sab jaanu       sab jaanu  Sakshi Yadav  lmhzpFfuWKc   \n",
       "\n",
       "     key_word_extracted                                            details  \\\n",
       "0                     0  n have come well in the morning time very much...   \n",
       "1                     0  st of our sindhis you have heard that dal dish...   \n",
       "2                     0  at dal dish in sindhis people feel very good a...   \n",
       "3                     0  i rice is ok these three or four things are ma...   \n",
       "4                     0  add ok so the lentils go but you have a little...   \n",
       "..                  ...                                                ...   \n",
       "477                   0  you will find hawa mahal and in the hand side ...   \n",
       "478                   0  d there i did not do the bark there how cannot...   \n",
       "479                   0  o now we have come to jaipurs main work get\\na...   \n",
       "480                   0   age\\nwell\\nbecause there is no many markets h...   \n",
       "481                   0  what new has come\\nho lak hai jaa ja jaane dat...   \n",
       "\n",
       "                                            clean_test  \n",
       "0    come morning time breakfast center breakfast g...  \n",
       "1    sindhis hear dal dish sindhis people feel good...  \n",
       "2    dal dish sindhis people feel good absolutely k...  \n",
       "3    rice thing maintain morning pricing hai pri si...  \n",
       "4    add lentil little bit chickpea onion tamarind ...  \n",
       "..                                                 ...  \n",
       "477  find hawa mahal hand left jori bazaar call lan...  \n",
       "478  bark look good mirror good lagar hai 450 last ...  \n",
       "479  come jaipurs main work miss turban tie come de...  \n",
       "480  age market link confusing bapu bazaar jauri ma...  \n",
       "481  new come lak hai jaa jaane dat krav hey pity z...  \n",
       "\n",
       "[482 rows x 7 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info['clean_test'] = df_base_info['details'].apply(process_text)\n",
    "df_base_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "12bc82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info = df_base_info.merge(df_entity_info,on=['lemma_word'],how='inner')\n",
    "landmarks = df_base_info[df_base_info['ENT_max'].isin(['LANDMARK','AREA'])].copy().reset_index().drop(columns='index',axis=1)\n",
    "food = df_base_info[df_base_info['ENT_max'].isin(['FOOD','FOOD SHOP'])].copy().reset_index().drop(columns='index',axis=1)\n",
    "# df_base_info = df_base_info[df_base_info['ENT_max'].isin(['LANDMARK','AREA','FOOD','FOOD SHOP'])].copy().reset_index().drop(columns='index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3dee7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info = df_base_info[df_base_info['ENT_max'].isin(['LANDMARK','AREA','FOOD','FOOD SHOP'])].copy().reset_index().drop(columns='index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90b0c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# cv = CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=0.28)\n",
    "# dim = cv.fit_transform(food['clean_test'])\n",
    "# cv_df = pd.DataFrame(dim.toarray(),columns=cv.get_feature_names_out())\n",
    "# print(cv_df.columns)\n",
    "# cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626cca3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e29b7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['actually', 'amazing', 'bada', 'bhandar', 'big', 'bit', 'chaat',\n",
      "       'cheese', 'chilli', 'chocolate', 'chutney', 'cream', 'day', 'different',\n",
      "       'eat', 'enjoy', 'famous', 'feel', 'flavor', 'flour', 'food', 'friend',\n",
      "       'garlic', 'ghee', 'hot', 'ice', 'ice cream', 'inside', 'kachori',\n",
      "       'kind', 'know', 'lassi', 'let', 'light', 'little', 'little bit', 'look',\n",
      "       'lot', 'love', 'mahavir', 'mahavir rabri', 'masala', 'milk', 'mirchi',\n",
      "       'nice', 'old', 'onion', 'paneer', 'people', 'plate', 'popular',\n",
      "       'potato', 'rabri', 'rajasthani', 'restaurant', 'samosa', 'shop',\n",
      "       'simple', 'size', 'small', 'special', 'specialty', 'start', 'sweet',\n",
      "       'taste', 'tell', 'think', 'time', 'variety', 'want', 'year'],\n",
      "      dtype='object')\n",
      "Index(['200', '400', '500', '900', 'absolutely', 'accord', 'address', 'albert',\n",
      "       'albert hall', 'amazing',\n",
      "       ...\n",
      "       'wind', 'window', 'work', 'world', 'world large', 'worth', 'year',\n",
      "       'year old', 'yeh', 'yes'],\n",
      "      dtype='object', length=340)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "root = os.getcwd()\n",
    "model_path = os.path.join(root.split('notebooks')[0], 'models')\n",
    "\n",
    "\n",
    "tvid_loc = TfidfVectorizer(stop_words='english',ngram_range=(1,2),max_df=0.2,min_df=0.05)\n",
    "X_locations = tvid_loc.fit_transform(landmarks['clean_test'])\n",
    "tvid_loc_df = pd.DataFrame(X_locations.toarray(),columns=tvid_loc.get_feature_names_out())\n",
    "\n",
    "with open(model_path+'/tvid_loc.pkl', 'wb') as file:\n",
    "    # Serialize and save the model to the file\n",
    "    pickle.dump(tvid_loc, file)\n",
    "\n",
    "\n",
    "tvid_food = TfidfVectorizer(stop_words='english',ngram_range=(1,2),max_df=0.3,min_df=0.15)\n",
    "X_food = tvid_food.fit_transform(food['clean_test'])\n",
    "tvid_food_df = pd.DataFrame(X_food.toarray(),columns=tvid_food.get_feature_names_out())\n",
    "\n",
    "with open(model_path+'/tvid_food.pkl', 'wb') as file:\n",
    "    # Serialize and save the model to the file\n",
    "    pickle.dump(tvid_food, file)\n",
    "\n",
    "\n",
    "# tvid_keyWords = TfidfVectorizer(stop_words='english',ngram_range=(1,2),min_df=0.15)\n",
    "# X_keyWords = tvid_keyWords.fit_transform(df_base_info['clean_test'])\n",
    "# tvid_keyWords_df = pd.DataFrame(X_keyWords.toarray(),columns=tvid_keyWords.get_feature_names_out())\n",
    "\n",
    "print(tvid_food_df.columns)\n",
    "print(tvid_loc_df.columns)\n",
    "# print(tvid_keyWords_df.columns)\n",
    "\n",
    "# df_base_info = pd.concat([df_base_info,tvid_keyWords_df],axis=1)\n",
    "# df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ad2a4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = pd.DataFrame(H,index=['Snaks','Evening','Desert','Tikka'])\n",
    "# W = pd.DataFrame(W,columns=['Snaks','Evening','Desert','Tikka'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "89640e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\randomProjects\\\\wanderly.ai\\\\models'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "787d62d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 2261 stored elements and shape (161, 71)>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "187ccfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['restaurant', 'experience', 'different', 'eat', 'shop', 'coffee', 'thing', 'food', 'plate', 'kachori']\n",
      "Topic 2: ['gate', 'bazaar', 'market', 'bapu', 'left', 'bag', 'right', 'new', 'tell', 'shop']\n",
      "Topic 3: ['amer', 'build', 'amer fort', 'courtyard', 'king', 'jaigarh', 'jaigarh fort', 'nahargarh', 'entry', 'sheesh']\n",
      "Topic 4: ['hawa', 'hawa mahal', 'floor', 'window', 'second', 'know', 'royal', 'maharaja', 'city palace', 'singh']\n",
      "Topic 5: ['mandir', 'park', 'ganesh', 'visit jaipur', 'white', 'world', 'gate', 'deewane', 'jawahar', 'deewane khas']\n",
      "Topic 6: ['garden', 'museum', 'hall museum', 'albert hall', 'albert', 'hall', 'apart', 'roam', 'build', 'know']\n",
      "Topic 7: ['sun', 'jantar', 'mantar', 'city', 'jantar mantar', 'lake', 'water', 'view', 'jaipur city', 'jal']\n",
      "Topic 1: ['masala', 'think', 'feel', 'time', 'bit', 'little', 'lot', 'little bit', 'different', 'actually']\n",
      "Topic 2: ['kachori', 'samosa', 'sweet', 'onion', 'mirchi', 'shop', 'bada', 'big', 'famous', 'lassi']\n",
      "Topic 3: ['year', 'rabri', 'start', 'food', 'eat', 'rajasthani', 'let', 'mahavir', 'cheese', 'inside']\n",
      "Topic 4: ['cream', 'ice', 'ice cream', 'chaat', 'variety', 'flavor', 'shop', 'bhandar', 'old', 'lassi']\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidIndexError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m food = pd.concat([food,W_food],axis=\u001b[32m1\u001b[39m)\n\u001b[32m     34\u001b[39m landmarks = pd.concat([landmarks,W_locations],axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m df_base_info = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfood\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.fillna(\u001b[32m0\u001b[39m)\n\u001b[32m     37\u001b[39m df_base_info.head(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jatin Arora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jatin Arora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    678\u001b[39m         obj_labels = obj.axes[\u001b[32m1\u001b[39m - ax]\n\u001b[32m    679\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels.equals(obj_labels):\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m             indexers[ax] = \u001b[43mobj_labels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m    684\u001b[39m new_data = concatenate_managers(\n\u001b[32m    685\u001b[39m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m.new_axes, concat_axis=\u001b[38;5;28mself\u001b[39m.bm_axis, copy=\u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m    686\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jatin Arora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3885\u001b[39m, in \u001b[36mIndex.get_indexer\u001b[39m\u001b[34m(self, target, method, limit, tolerance)\u001b[39m\n\u001b[32m   3882\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_method(method, limit, tolerance)\n\u001b[32m   3884\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_as_unique:\n\u001b[32m-> \u001b[39m\u001b[32m3885\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m._requires_unique_msg)\n\u001b[32m   3887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) == \u001b[32m0\u001b[39m:\n\u001b[32m   3888\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array([], dtype=np.intp)\n",
      "\u001b[31mInvalidIndexError\u001b[39m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_model_loc = NMF(n_components=7)\n",
    "W_locations = nmf_model_loc.fit_transform(X_locations)\n",
    "H_locations = nmf_model_loc.components_\n",
    "\n",
    "nmf_model_food = NMF(n_components=4)\n",
    "W_food = nmf_model_food.fit_transform(X_food)\n",
    "H_food = nmf_model_food.components_\n",
    "\n",
    "with open(model_path+'/nmf_model_loc.pkl', 'wb') as file:\n",
    "    # Serialize and save the model to the file\n",
    "    pickle.dump(nmf_model_loc, file)\n",
    "\n",
    "with open(model_path+'/nmf_model_food.pkl', 'wb') as file:\n",
    "    # Serialize and save the model to the file\n",
    "    pickle.dump(nmf_model_food, file)\n",
    "\n",
    "\n",
    "def display_topic(H,tvid):\n",
    "    for topic_num, topic_array in enumerate(H):\n",
    "        top_features = topic_array.argsort()[::-1][:10]\n",
    "        top_words = [tvid.get_feature_names_out()[i] for i in top_features]\n",
    "        print(f\"Topic {topic_num +1}: {top_words}\")\n",
    "\n",
    "display_topic(H_locations,tvid_loc)\n",
    "display_topic(H_food,tvid_food)\n",
    "# H = pd.DataFrame(H,index=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "W_locations = pd.DataFrame(W_locations,columns=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "\n",
    "# H = pd.DataFrame(H,index=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "W_food = pd.DataFrame(W_food,columns=['Snaks','Evening','Desert','Tikka'])\n",
    "\n",
    "food = pd.concat([food,W_food],axis=1)\n",
    "landmarks = pd.concat([landmarks,W_locations],axis=1)\n",
    "\n",
    "df_base_info = pd.concat([landmarks,food]).fillna(0)\n",
    "df_base_info.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "57837c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['big', 'come', 'day', 'different', 'eat', 'famous', 'feel', 'food',\n",
      "       'fort', 'good', 'inside', 'jaipur', 'kachori', 'know', 'let', 'like',\n",
      "       'little', 'look', 'lot', 'mahal', 'old', 'palace', 'people', 'place',\n",
      "       'right', 'shop', 'small', 'special', 'temple', 'thing', 'time', 'try',\n",
      "       'visit', 'want', 'year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tvid_keyWords = TfidfVectorizer(stop_words='english',ngram_range=(1,2),min_df=0.15)\n",
    "X_keyWords = tvid_keyWords.fit_transform(df_base_info['clean_test'])\n",
    "tvid_keyWords_df = pd.DataFrame(X_keyWords.toarray(),columns=tvid_keyWords.get_feature_names_out())\n",
    "\n",
    "print(tvid_keyWords_df.columns)\n",
    "\n",
    "root = os.getcwd()\n",
    "model_path = os.path.join(root.split('notebooks')[0], 'models')\n",
    "with open(model_path+'/tvid_keyWords.pkl', 'wb') as file:\n",
    "    # Serialize and save the model to the file\n",
    "    pickle.dump(tvid_keyWords, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f99fa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info = df_base_info.reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1472c28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>rating</th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>Forts</th>\n",
       "      <th>...</th>\n",
       "      <th>shop</th>\n",
       "      <th>small</th>\n",
       "      <th>special</th>\n",
       "      <th>temple</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>try</th>\n",
       "      <th>visit</th>\n",
       "      <th>want</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai pri</td>\n",
       "      <td>hai pri</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>i rice is ok these three or four things are ma...</td>\n",
       "      <td>rice thing maintain morning pricing hai pri si...</td>\n",
       "      <td>19.837405</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.084501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.915543</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allredi trai</td>\n",
       "      <td>allredi trai</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>e lentil and of tapre mirchi will keep my effo...</td>\n",
       "      <td>lentil tapre mirchi effort basically let thing...</td>\n",
       "      <td>19.837405</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.072218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lemma_word          word suggested_by     video_id  key_word_extracted  \\\n",
       "0       hai pri       hai pri   Singhsman   p0MvovsCxCk                   0   \n",
       "1  allredi trai  allredi trai   Singhsman   p0MvovsCxCk                   0   \n",
       "\n",
       "                                             details  \\\n",
       "0  i rice is ok these three or four things are ma...   \n",
       "1  e lentil and of tapre mirchi will keep my effo...   \n",
       "\n",
       "                                          clean_test     rating   ENT_max  \\\n",
       "0  rice thing maintain morning pricing hai pri si...  19.837405  LANDMARK   \n",
       "1  lentil tapre mirchi effort basically let thing...  19.837405  LANDMARK   \n",
       "\n",
       "      Forts  ...  shop  small  special  temple     thing  time      try  \\\n",
       "0  0.084501  ...   0.0    0.0      0.0     0.0  0.915543   0.0  0.00000   \n",
       "1  0.072218  ...   0.0    0.0      0.0     0.0  0.476420   0.0  0.51575   \n",
       "\n",
       "   visit  want  year  \n",
       "0    0.0   0.0   0.0  \n",
       "1    0.0   0.0   0.0  \n",
       "\n",
       "[2 rows x 55 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info = pd.concat([df_base_info,tvid_keyWords_df],axis=1)\n",
    "df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ebab69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmarks = pd.concat([landmarks,W],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c163ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# food = pd.concat([food,W],axis=1)\n",
    "# landmarks = pd.concat([landmarks,W],axis=1)\n",
    "# df_base_info = pd.concat([landmarks,food]).fillna(0)\n",
    "# df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d79f3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_info = pd.concat([landmarks,food]).fillna(0)\n",
    "# df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ad3cda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_rateing(max_r,min_r,current_r):\n",
    "#     return (current_r - min_r)/(max_r + min_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f1f6415a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>min_category_rating</th>\n",
       "      <th>max_category_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AREA</td>\n",
       "      <td>2.372318</td>\n",
       "      <td>43.837405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>2.660633</td>\n",
       "      <td>35.610902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOOD SHOP</td>\n",
       "      <td>2.456626</td>\n",
       "      <td>35.610902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>2.372318</td>\n",
       "      <td>39.610902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ENT_max  min_category_rating  max_category_rating\n",
       "0       AREA             2.372318            43.837405\n",
       "1       FOOD             2.660633            35.610902\n",
       "2  FOOD SHOP             2.456626            35.610902\n",
       "3   LANDMARK             2.372318            39.610902"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_category_rateing = df_base_info.groupby(by='ENT_max')['rating'].agg(['min','max']).reset_index().rename(columns={'min':'min_category_rating','max':'max_category_rating'})\n",
    "min_max_category_rateing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "477ee73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>rating</th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>Forts</th>\n",
       "      <th>...</th>\n",
       "      <th>small</th>\n",
       "      <th>special</th>\n",
       "      <th>temple</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>try</th>\n",
       "      <th>visit</th>\n",
       "      <th>want</th>\n",
       "      <th>year</th>\n",
       "      <th>normalize_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai pri</td>\n",
       "      <td>hai pri</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>i rice is ok these three or four things are ma...</td>\n",
       "      <td>rice thing maintain morning pricing hai pri si...</td>\n",
       "      <td>19.837405</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.084501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.915543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allredi trai</td>\n",
       "      <td>allredi trai</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>e lentil and of tapre mirchi will keep my effo...</td>\n",
       "      <td>lentil tapre mirchi effort basically let thing...</td>\n",
       "      <td>19.837405</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.072218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allredi</td>\n",
       "      <td>allredi</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>e lentil and of tapre mirchi will keep my effo...</td>\n",
       "      <td>lentil tapre mirchi effort basically let thing...</td>\n",
       "      <td>20.837405</td>\n",
       "      <td>AREA</td>\n",
       "      <td>0.072218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nana ji</td>\n",
       "      <td>nana ji</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>name sir gulshan bhatia sir our father started...</td>\n",
       "      <td>sir gulshan bhatia sir father start paneer goo...</td>\n",
       "      <td>19.837405</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.044190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>vaccines</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>something when the pure cheese was with you a...</td>\n",
       "      <td>pure cheese blessing cheese introduction vacci...</td>\n",
       "      <td>19.837405</td>\n",
       "      <td>AREA</td>\n",
       "      <td>0.049754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.377953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>vaidyashala</td>\n",
       "      <td>vaidyashala</td>\n",
       "      <td>Yatra Mitra</td>\n",
       "      <td>dcRh1zTPnDQ</td>\n",
       "      <td>0</td>\n",
       "      <td>was on this lovely vaidyalaya by included in ...</td>\n",
       "      <td>lovely vaidyalaya include world heritage list ...</td>\n",
       "      <td>4.017759</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>ganesh poll</td>\n",
       "      <td>ganesh poll</td>\n",
       "      <td>Yatra Mitra</td>\n",
       "      <td>dcRh1zTPnDQ</td>\n",
       "      <td>0</td>\n",
       "      <td>ve also been shot here see you very special vi...</td>\n",
       "      <td>shoot special view look view ganesh poll win h...</td>\n",
       "      <td>3.017759</td>\n",
       "      <td>FOOD SHOP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.743404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>rajo rani</td>\n",
       "      <td>rajo rani</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "      <td>s\\nappreciation explore ok such bags find a lo...</td>\n",
       "      <td>appreciation explore bag find lot bag like bag...</td>\n",
       "      <td>2.456626</td>\n",
       "      <td>FOOD SHOP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>rahi</td>\n",
       "      <td>rahi</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "      <td>ight prints show the dress that took the dress...</td>\n",
       "      <td>ight print dress take dress like dear brother ...</td>\n",
       "      <td>2.456626</td>\n",
       "      <td>FOOD SHOP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>agar</td>\n",
       "      <td>agar</td>\n",
       "      <td>Sakshi Yadav</td>\n",
       "      <td>lmhzpFfuWKc</td>\n",
       "      <td>0</td>\n",
       "      <td>d there i did not do the bark there how cannot...</td>\n",
       "      <td>bark look good mirror good lagar hai 450 last ...</td>\n",
       "      <td>3.456626</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112799</td>\n",
       "      <td>0.058014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055389</td>\n",
       "      <td>0.053731</td>\n",
       "      <td>0.225597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lemma_word          word  suggested_by     video_id  \\\n",
       "0         hai pri       hai pri    Singhsman   p0MvovsCxCk   \n",
       "1    allredi trai  allredi trai    Singhsman   p0MvovsCxCk   \n",
       "2         allredi       allredi    Singhsman   p0MvovsCxCk   \n",
       "3         nana ji       nana ji    Singhsman   p0MvovsCxCk   \n",
       "4         vaccine      vaccines    Singhsman   p0MvovsCxCk   \n",
       "..            ...           ...           ...          ...   \n",
       "442   vaidyashala   vaidyashala   Yatra Mitra  dcRh1zTPnDQ   \n",
       "443   ganesh poll   ganesh poll   Yatra Mitra  dcRh1zTPnDQ   \n",
       "444     rajo rani     rajo rani  Sakshi Yadav  lmhzpFfuWKc   \n",
       "445          rahi          rahi  Sakshi Yadav  lmhzpFfuWKc   \n",
       "446          agar          agar  Sakshi Yadav  lmhzpFfuWKc   \n",
       "\n",
       "     key_word_extracted                                            details  \\\n",
       "0                     0  i rice is ok these three or four things are ma...   \n",
       "1                     0  e lentil and of tapre mirchi will keep my effo...   \n",
       "2                     0  e lentil and of tapre mirchi will keep my effo...   \n",
       "3                     0  name sir gulshan bhatia sir our father started...   \n",
       "4                     0   something when the pure cheese was with you a...   \n",
       "..                  ...                                                ...   \n",
       "442                   0   was on this lovely vaidyalaya by included in ...   \n",
       "443                   0  ve also been shot here see you very special vi...   \n",
       "444                   0  s\\nappreciation explore ok such bags find a lo...   \n",
       "445                   0  ight prints show the dress that took the dress...   \n",
       "446                   0  d there i did not do the bark there how cannot...   \n",
       "\n",
       "                                            clean_test     rating    ENT_max  \\\n",
       "0    rice thing maintain morning pricing hai pri si...  19.837405   LANDMARK   \n",
       "1    lentil tapre mirchi effort basically let thing...  19.837405   LANDMARK   \n",
       "2    lentil tapre mirchi effort basically let thing...  20.837405       AREA   \n",
       "3    sir gulshan bhatia sir father start paneer goo...  19.837405   LANDMARK   \n",
       "4    pure cheese blessing cheese introduction vacci...  19.837405       AREA   \n",
       "..                                                 ...        ...        ...   \n",
       "442  lovely vaidyalaya include world heritage list ...   4.017759       FOOD   \n",
       "443  shoot special view look view ganesh poll win h...   3.017759  FOOD SHOP   \n",
       "444  appreciation explore bag find lot bag like bag...   2.456626  FOOD SHOP   \n",
       "445  ight print dress take dress like dear brother ...   2.456626  FOOD SHOP   \n",
       "446  bark look good mirror good lagar hai 450 last ...   3.456626       FOOD   \n",
       "\n",
       "        Forts  ...     small   special  temple     thing      time       try  \\\n",
       "0    0.084501  ...  0.000000  0.000000     0.0  0.915543  0.000000  0.000000   \n",
       "1    0.072218  ...  0.000000  0.000000     0.0  0.476420  0.000000  0.515750   \n",
       "2    0.072218  ...  0.000000  0.000000     0.0  0.442900  0.000000  0.479462   \n",
       "3    0.044190  ...  0.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
       "4    0.049754  ...  0.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
       "..        ...  ...       ...       ...     ...       ...       ...       ...   \n",
       "442  0.000000  ...  0.000000  0.000000     0.0  0.000000  0.311179  0.000000   \n",
       "443  0.000000  ...  0.000000  0.743404     0.0  0.000000  0.000000  0.000000   \n",
       "444  0.000000  ...  0.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
       "445  0.000000  ...  0.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
       "446  0.000000  ...  0.112799  0.058014     0.0  0.000000  0.000000  0.055389   \n",
       "\n",
       "        visit      want  year  normalize_rating  \n",
       "0    0.000000  0.000000   0.0          0.416002  \n",
       "1    0.000000  0.000000   0.0          0.416002  \n",
       "2    0.000000  0.000000   0.0          0.399593  \n",
       "3    0.000000  0.000000   0.0          0.416002  \n",
       "4    0.000000  0.000000   1.0          0.377953  \n",
       "..        ...       ...   ...               ...  \n",
       "442  0.000000  0.341664   0.0          0.035460  \n",
       "443  0.000000  0.000000   0.0          0.014740  \n",
       "444  0.000000  0.503067   0.0          0.000000  \n",
       "445  0.000000  0.000000   0.0          0.000000  \n",
       "446  0.053731  0.225597   0.0          0.020799  \n",
       "\n",
       "[447 rows x 56 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info = df_base_info.merge(min_max_category_rateing,on='ENT_max')\n",
    "df_base_info['normalize_rating'] = (df_base_info['rating'] - df_base_info['min_category_rating'])/(df_base_info['max_category_rating'] + df_base_info['min_category_rating'])\n",
    "df_base_info.drop(['min_category_rating','max_category_rating'],axis=1,inplace=True)\n",
    "df_base_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25606329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4bdddf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lemma_word', 'word', 'suggested_by', 'video_id', 'key_word_extracted',\n",
       "       'details', 'clean_test', 'rating', 'ENT_max', 'Forts', 'Generic',\n",
       "       'Temple', 'Temples & Architectur', 'Scientific Monuments',\n",
       "       'Lakeside Forts', 'Views', 'Snaks', 'Evening', 'Desert', 'Tikka', 'big',\n",
       "       'come', 'day', 'different', 'eat', 'famous', 'feel', 'food', 'fort',\n",
       "       'good', 'inside', 'jaipur', 'kachori', 'know', 'let', 'like', 'little',\n",
       "       'look', 'lot', 'mahal', 'old', 'palace', 'people', 'place', 'right',\n",
       "       'shop', 'small', 'special', 'temple', 'thing', 'time', 'try', 'visit',\n",
       "       'want', 'year', 'normalize_rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "262f8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info.to_csv('D:/randomProjects/wanderly.ai/data/Parquet/final_loc_food_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "551e388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_col = [i for i in df_base_info.columns if i not in ['lemma_word', 'word', 'suggested_by', \n",
    "                                                        'video_id','key_word_extracted','details', \n",
    "                                                        'clean_test', 'rating', 'ENT_max','normalize_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79edc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8418b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_base_info[req_col]\n",
    "Y = df_base_info[['lemma_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6587bdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhat excites you most about traveling?\\nWhich of these travel quotes resonates with you the most?\\nIf you had a 3-day break, what would you do?\\nWhat type of destinations do you find most appealing?\\nWhat kind of experiences do you look for while traveling?\\nWhat do you often post or talk about after your trips?\\nWhich of these describe you best when traveling?\\nWhat's the first thing you usually do after reaching a new place?\\nDescribe your dream travel day in 2–3 sentences.\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "What excites you most about traveling?\n",
    "Which of these travel quotes resonates with you the most?\n",
    "If you had a 3-day break, what would you do?\n",
    "What type of destinations do you find most appealing?\n",
    "What kind of experiences do you look for while traveling?\n",
    "What do you often post or talk about after your trips?\n",
    "Which of these describe you best when traveling?\n",
    "What's the first thing you usually do after reaching a new place?\n",
    "Describe your dream travel day in 2–3 sentences.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cb8d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = ['I like to trave to find me, enjoy, capture the movement',\n",
    "'Like to explore the culture, heritage and local food',\n",
    "'relax chill with friends',\n",
    "'peacefull place calm and relaxing',\n",
    "'chill good local food instagram worthy locations, vloging',\n",
    "'Explorer – I love wandering and getting lost',\n",
    "'Explore the surroundings on foot',\n",
    "'Dream travel worule be lost in the movement, relax, local food, chill vibes, haritage, mountain person but \\\n",
    "also like beaches and water side resorts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5107a137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to trave to find me, enjoy, capture the movement Like to explore the culture, heritage and local food relax chill with friends peacefull place calm and relaxing chill good local food instagram worthy locations, vloging Explorer – I love wandering and getting lost Explore the surroundings on foot Dream travel worule be lost in the movement, relax, local food, chill vibes, haritage, mountain person but also like beaches and water side resorts\n"
     ]
    }
   ],
   "source": [
    "user_input = ' '.join(user_input)\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd6507a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jatin</td>\n",
       "      <td>I like to trave to find me, enjoy, capture the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name                                               desc\n",
       "0  Jatin  I like to trave to find me, enjoy, capture the..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = pd.DataFrame({'name':['Jatin'],'desc':[user_input]})\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ee9867c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "      <th>clean_dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jatin</td>\n",
       "      <td>I like to trave to find me, enjoy, capture the...</td>\n",
       "      <td>like trave find enjoy capture movement like ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name                                               desc  \\\n",
       "0  Jatin  I like to trave to find me, enjoy, capture the...   \n",
       "\n",
       "                                           clean_dec  \n",
       "0  like trave find enjoy capture movement like ex...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user['clean_dec'] = user['desc'].apply(lambda x: process_text(cleaning_sentence(x)))\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f9f767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_locations = tvid_loc.transform(user['clean_dec'])\n",
    "tvid_loc_df = pd.DataFrame(X_locations.toarray(),columns=tvid_loc.get_feature_names_out())\n",
    "\n",
    "X_food = tvid_food.transform(user['clean_dec'])\n",
    "tvid_food_df = pd.DataFrame(X_food.toarray(),columns=tvid_food.get_feature_names_out())\n",
    "\n",
    "X_keyWords = tvid_keyWords.transform(user['clean_dec'])\n",
    "tvid_keyWords_df = pd.DataFrame(X_keyWords.toarray(),columns=tvid_keyWords.get_feature_names_out())\n",
    "# tvid_keyWords_df = pd.DataFrame(X_keyWords.toarray(),columns=tvid_keyWords.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "257e2515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>big</th>\n",
       "      <th>come</th>\n",
       "      <th>day</th>\n",
       "      <th>different</th>\n",
       "      <th>eat</th>\n",
       "      <th>famous</th>\n",
       "      <th>feel</th>\n",
       "      <th>food</th>\n",
       "      <th>fort</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>shop</th>\n",
       "      <th>small</th>\n",
       "      <th>special</th>\n",
       "      <th>temple</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>try</th>\n",
       "      <th>visit</th>\n",
       "      <th>want</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   big  come  day  different  eat  famous  feel     food  fort      good  ...  \\\n",
       "0  0.0   0.0  0.0        0.0  0.0     0.0   0.0  0.77257   0.0  0.195676  ...   \n",
       "\n",
       "   shop  small  special  temple  thing  time  try  visit  want  year  \n",
       "0   0.0    0.0      0.0     0.0    0.0   0.0  0.0    0.0   0.0   0.0  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvid_keyWords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f36af3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_locations = nmf_model_loc.transform(X_locations)\n",
    "H_locations = nmf_model_loc.components_\n",
    "\n",
    "W_food = nmf_model_food.transform(X_food)\n",
    "H_food = nmf_model_food.components_\n",
    "\n",
    "W_locations = pd.DataFrame(W_locations,columns=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "\n",
    "# H = pd.DataFrame(H,index=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "W_food = pd.DataFrame(W_food,columns=['Snaks','Evening','Desert','Tikka'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57c7d5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(W_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c62ce00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(W_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5609d571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "      <th>clean_dec</th>\n",
       "      <th>big</th>\n",
       "      <th>come</th>\n",
       "      <th>day</th>\n",
       "      <th>different</th>\n",
       "      <th>eat</th>\n",
       "      <th>famous</th>\n",
       "      <th>feel</th>\n",
       "      <th>...</th>\n",
       "      <th>Generic</th>\n",
       "      <th>Temple</th>\n",
       "      <th>Temples &amp; Architectur</th>\n",
       "      <th>Scientific Monuments</th>\n",
       "      <th>Lakeside Forts</th>\n",
       "      <th>Views</th>\n",
       "      <th>Snaks</th>\n",
       "      <th>Evening</th>\n",
       "      <th>Desert</th>\n",
       "      <th>Tikka</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jatin</td>\n",
       "      <td>I like to trave to find me, enjoy, capture the...</td>\n",
       "      <td>like trave find enjoy capture movement like ex...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.015045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.182176</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name                                               desc  \\\n",
       "0  Jatin  I like to trave to find me, enjoy, capture the...   \n",
       "\n",
       "                                           clean_dec  big  come  day  \\\n",
       "0  like trave find enjoy capture movement like ex...  0.0   0.0  0.0   \n",
       "\n",
       "   different  eat  famous  feel  ...   Generic    Temple  \\\n",
       "0        0.0  0.0     0.0   0.0  ...  0.003722  0.015045   \n",
       "\n",
       "   Temples & Architectur  Scientific Monuments  Lakeside Forts     Views  \\\n",
       "0                    0.0                   0.0             0.0  0.062283   \n",
       "\n",
       "   Snaks  Evening    Desert  Tikka  \n",
       "0    0.0      0.0  0.182176    0.0  \n",
       "\n",
       "[1 rows x 49 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_transform = pd.concat([user,tvid_keyWords_df,W_locations,W_food],axis=1)\n",
    "user_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0fa3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_similar_objects(check,X,Y,top_n=5):\n",
    "    common = [i for i in X.columns if i in check.columns]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X[common])\n",
    "    user_scaled = scaler.transform(check[common])\n",
    "    \n",
    "    # Compute cosine similarity between user vector and all item vectors\n",
    "    similarity_scores = cosine_similarity(user_scaled, X[common])[0]\n",
    "\n",
    "    # Add scores to DataFrame\n",
    "    Y['similarity'] = similarity_scores\n",
    "\n",
    "    # Sort top recommendations\n",
    "    top_recommendations = Y.sort_values(by='similarity', ascending=False).head(10)\n",
    "    return top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e9bb7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin Arora\\AppData\\Local\\Temp\\ipykernel_30724\\2801194235.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Y['similarity'] = similarity_scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sodala</td>\n",
       "      <td>0.712143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>gaurav tar</td>\n",
       "      <td>0.674760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>shweta</td>\n",
       "      <td>0.620578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>gulab ji</td>\n",
       "      <td>0.596382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>mahavir rabri bhandar</td>\n",
       "      <td>0.553632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>tride</td>\n",
       "      <td>0.533788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>mi road</td>\n",
       "      <td>0.531921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>dhaba</td>\n",
       "      <td>0.526955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>rawat mishthan bhandar</td>\n",
       "      <td>0.509835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>aamir food</td>\n",
       "      <td>0.494557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 lemma_word  similarity\n",
       "7                    sodala    0.712143\n",
       "188              gaurav tar    0.674760\n",
       "166                  shweta    0.620578\n",
       "301                gulab ji    0.596382\n",
       "370   mahavir rabri bhandar    0.553632\n",
       "228                   tride    0.533788\n",
       "233                 mi road    0.531921\n",
       "323                   dhaba    0.526955\n",
       "426  rawat mishthan bhandar    0.509835\n",
       "338              aamir food    0.494557"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_objects(user_transform,X,Y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "559164e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>rating</th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>Forts</th>\n",
       "      <th>...</th>\n",
       "      <th>small</th>\n",
       "      <th>special</th>\n",
       "      <th>temple</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>try</th>\n",
       "      <th>visit</th>\n",
       "      <th>want</th>\n",
       "      <th>year</th>\n",
       "      <th>normalize_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>sambar lake</td>\n",
       "      <td>sambar lake</td>\n",
       "      <td>Aniruddha Patil</td>\n",
       "      <td>Lv0PkSkKeSo</td>\n",
       "      <td>0</td>\n",
       "      <td>m jaipur a distance of 75 km but indias bigges...</td>\n",
       "      <td>jaipur distance india big salt settle water la...</td>\n",
       "      <td>6.372318</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208831</td>\n",
       "      <td>0.214811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.431709</td>\n",
       "      <td>0.095276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma_word         word     suggested_by     video_id  \\\n",
       "153  sambar lake  sambar lake  Aniruddha Patil  Lv0PkSkKeSo   \n",
       "\n",
       "     key_word_extracted                                            details  \\\n",
       "153                   0  m jaipur a distance of 75 km but indias bigges...   \n",
       "\n",
       "                                            clean_test    rating   ENT_max  \\\n",
       "153  jaipur distance india big salt settle water la...  6.372318  LANDMARK   \n",
       "\n",
       "        Forts  ...     small   special  temple  thing  time  try  visit  want  \\\n",
       "153  0.023057  ...  0.208831  0.214811     0.0    0.0   0.0  0.0    0.0   0.0   \n",
       "\n",
       "         year  normalize_rating  \n",
       "153  0.431709          0.095276  \n",
       "\n",
       "[1 rows x 56 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info[df_base_info['lemma_word']=='sambar lake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e20264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin Arora\\AppData\\Local\\Temp\\ipykernel_9820\\2013376519.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Y['similarity'] = similarity_scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>chowki dhani</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sodala</td>\n",
       "      <td>0.825258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>dhaba</td>\n",
       "      <td>0.771376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>nahargarh</td>\n",
       "      <td>0.639082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gulab ji</td>\n",
       "      <td>0.581485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>shankar samosa</td>\n",
       "      <td>0.564242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shimla tutter</td>\n",
       "      <td>0.540364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>maggi</td>\n",
       "      <td>0.537956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hawa mahal</td>\n",
       "      <td>0.537956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>tamarind sauce</td>\n",
       "      <td>0.523520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lemma_word  similarity\n",
       "40     chowki dhani    1.000000\n",
       "7            sodala    0.825258\n",
       "166           dhaba    0.771376\n",
       "56        nahargarh    0.639082\n",
       "8          gulab ji    0.581485\n",
       "153  shankar samosa    0.564242\n",
       "6     shimla tutter    0.540364\n",
       "177           maggi    0.537956\n",
       "16       hawa mahal    0.537956\n",
       "141  tamarind sauce    0.523520"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_objects(df_base_info[df_base_info['lemma_word']=='chowki dhani'],X,Y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afcb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pune</td>\n",
       "      <td>0.656606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>zenana deodi</td>\n",
       "      <td>0.613180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nana ji</td>\n",
       "      <td>0.607636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mubarak mahal</td>\n",
       "      <td>0.546297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>chaat</td>\n",
       "      <td>0.546149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>palace of reception</td>\n",
       "      <td>0.519179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>arun bhai</td>\n",
       "      <td>0.513964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hawa mahal</td>\n",
       "      <td>0.482137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>raman dosa</td>\n",
       "      <td>0.380070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>amer fort</td>\n",
       "      <td>0.370288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lemma_word  similarity\n",
       "27                  pune    0.656606\n",
       "85          zenana deodi    0.613180\n",
       "9                nana ji    0.607636\n",
       "49         mubarak mahal    0.546297\n",
       "180                chaat    0.546149\n",
       "50   palace of reception    0.519179\n",
       "195            arun bhai    0.513964\n",
       "28            hawa mahal    0.482137\n",
       "178           raman dosa    0.380070\n",
       "57             amer fort    0.370288"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_recommendations = Y.sort_values(by='similarity', ascending=False).head(10)\n",
    "top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import required modules after environment reset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the main function to recommend keywords\n",
    "def recommend_keywords_for_user(\n",
    "    user_input,\n",
    "    df_base_info,\n",
    "    tfidf_models,\n",
    "    nmf_models,\n",
    "    keyword_vectorizer,\n",
    "    process_text_func,\n",
    "    req_col,\n",
    "    topic_labels,\n",
    "    normalize=True,\n",
    "    top_n=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Full keyword recommendation pipeline from raw user input.\n",
    "\n",
    "    Args:\n",
    "        user_input (str): Raw user description or answers.\n",
    "        df_base_info (pd.DataFrame): DataFrame containing features and metadata.\n",
    "        tfidf_models (dict): Dictionary with 'location' and 'food' TfidfVectorizers.\n",
    "        nmf_models (dict): Dictionary with 'location' and 'food' NMF models.\n",
    "        keyword_vectorizer: TF-IDF vectorizer used for general keywords.\n",
    "        process_text_func (callable): Text cleaning and lemmatization function.\n",
    "        req_col (list): Final column names used in cosine similarity.\n",
    "        topic_labels (dict): {'location': [...], 'food': [...]}\n",
    "        normalize (bool): Whether to scale all features.\n",
    "        top_n (int): Number of recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Top N recommended keywords/entities.\n",
    "    \"\"\"\n",
    "    # 1. Clean and prepare user input\n",
    "    user_clean = process_text_func(user_input)\n",
    "\n",
    "    # 2. Transform into TF-IDF vectors\n",
    "    X_loc = tfidf_models['location'].transform([user_clean])\n",
    "    X_food = tfidf_models['food'].transform([user_clean])\n",
    "    X_key = keyword_vectorizer.transform([user_clean])\n",
    "\n",
    "    # 3. Apply NMF topic models\n",
    "    W_loc = nmf_models['location'].transform(X_loc)\n",
    "    W_food = nmf_models['food'].transform(X_food)\n",
    "\n",
    "    # 4. Build user feature vector\n",
    "    user_vector = np.concatenate([\n",
    "        W_loc[0],\n",
    "        W_food[0],\n",
    "        [0.5],  # normalize_rating neutral\n",
    "        X_key.toarray()[0][:len(req_col) - (len(W_loc[0]) + len(W_food[0]) + 1)]\n",
    "    ])\n",
    "    user_vector = pd.DataFrame([user_vector], columns=req_col[:len(user_vector)])\n",
    "\n",
    "    # 5. Feature scaling (optional)\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(df_base_info[req_col])\n",
    "        user_scaled = scaler.transform(user_vector)\n",
    "    else:\n",
    "        X_scaled = df_base_info[req_col].values\n",
    "        user_scaled = user_vector.values\n",
    "\n",
    "    # 6. Cosine similarity\n",
    "    similarity_scores = cosine_similarity(user_scaled, X_scaled)[0]\n",
    "    df_result = df_base_info.copy()\n",
    "    df_result['score'] = similarity_scores\n",
    "\n",
    "    # 7. Return top N\n",
    "    return df_result.sort_values(by='score', ascending=False).head(top_n)[\n",
    "        ['lemma_word', 'ENT_max', 'video_id', 'suggested_by', 'score']\n",
    "    ]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
