{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "51f50947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import string\n",
    "import contractions\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from googleapiclient.discovery import build\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d319bb1",
   "metadata": {},
   "source": [
    "### Key word Extractring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e7ec1160",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'AIzaSyBG7X8oJ1CYdPZd7F4gSo605Jf-EfD7IHM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f83ab507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_statistics(video_id): \n",
    "    # video_id = '7cPLbiblb84'\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "        request = youtube.videos().list(\n",
    "            part=['snippet','statistics'],\n",
    "            id=video_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "        stats = response['items'][0]['statistics']\n",
    "        viewCount = stats['viewCount'] if 'viewCount' in stats.keys() else 0\n",
    "        likeCount = stats['likeCount'] if 'likeCount' in stats.keys() else 0\n",
    "        commentCount = stats['commentCount'] if 'commentCount' in stats.keys() else 0 \n",
    "        channelId = response['items'][0]['snippet']['channelId']\n",
    "        channel_name = response['items'][0]['snippet']['channelTitle']\n",
    "\n",
    "        print(response['items'][0]['statistics'],channelId,channel_name)\n",
    "        \n",
    "        return int(viewCount),int(likeCount),int(commentCount),channel_name,channelId\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def get_channel_statistics(channelId):\n",
    "    try:\n",
    "        youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "        request_channel =  youtube.channels().list(\n",
    "            part=['statistics'],\n",
    "            id=channelId\n",
    "        )\n",
    "        response_channel = request_channel.execute()\n",
    "        subscriberCount= response_channel['items'][0]['statistics']['subscriberCount']\n",
    "        return int(subscriberCount)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def cleaning_sentence(text):\n",
    "    text = text.lower()\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    ans = contractions.fix(text).translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "    ans = ans.replace('music','')\n",
    "    ans = ans.replace(\"  \",' ')\n",
    "    ans = \" \".join(ans.split())\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c4b95624",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video_ids = [\n",
    "    'p0MvovsCxCk',\n",
    "    '7cPLbiblb84',\n",
    "    'rm_j6O8y148',\n",
    "    'Lv0PkSkKeSo',\n",
    "    '5zA6OFpkPe0',\n",
    "    'Oz18u64bM8I',\n",
    "    'p0MvovsCxCk',\n",
    "    '7cPLbiblb84'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1c7663e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0MvovsCxCk\n",
      "{'viewCount': '281079', 'favoriteCount': '0', 'commentCount': '97'} UCoCU-RvDqV1JgmbRgh79UTQ Singhsman \n",
      "7cPLbiblb84\n",
      "{'viewCount': '2031633', 'likeCount': '16193', 'favoriteCount': '0', 'commentCount': '864'} UC_P_sA6Jf3iSsFueMwIP3vg TheSocialTraveller\n",
      "rm_j6O8y148\n",
      "{'viewCount': '189021', 'likeCount': '2490', 'favoriteCount': '0', 'commentCount': '91'} UCQ0X2x6lozB7qG30ctFeUiA Saturday Shooters\n",
      "Lv0PkSkKeSo\n",
      "{'viewCount': '116809', 'likeCount': '2215', 'favoriteCount': '0', 'commentCount': '188'} UC2N5r2FvEOiDPKvaLW9rivw Aniruddha Patil\n",
      "5zA6OFpkPe0\n",
      "{'viewCount': '10198', 'likeCount': '201', 'favoriteCount': '0', 'commentCount': '33'} UCCq9CLWwVP4fdYy2N_ypTjg Sisters vs Globe\n",
      "Oz18u64bM8I\n",
      "{'viewCount': '9929', 'likeCount': '169', 'favoriteCount': '0', 'commentCount': '36'} UC-w8ULGFYLrjbdsZ6twWO3Q Travel Tales\n",
      "p0MvovsCxCk\n",
      "{'viewCount': '281079', 'favoriteCount': '0', 'commentCount': '97'} UCoCU-RvDqV1JgmbRgh79UTQ Singhsman \n",
      "7cPLbiblb84\n",
      "{'viewCount': '2031636', 'likeCount': '16193', 'favoriteCount': '0', 'commentCount': '864'} UC_P_sA6Jf3iSsFueMwIP3vg TheSocialTraveller\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma_word         word suggested_by     video_id  key_word_extracted\n",
       "0   sindhi dal   sindhi dal   Singhsman   p0MvovsCxCk                   0\n",
       "1  kudi chhola  kudi chhola   Singhsman   p0MvovsCxCk                   0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "model_path = os.path.join(root.split('notebooks')[0], 'models', 'ner_mod','model-best')\n",
    "nlp_ner = spacy.load(model_path)\n",
    "nlp     = spacy.load(\"en_core_web_sm\")\n",
    "ner     = dict()\n",
    "    \n",
    "for video_id in test_video_ids:\n",
    "    try:\n",
    "        print(video_id)\n",
    "        \n",
    "        viewCount,likeCount,commentCount,channel_name,channelId = get_video_statistics(video_id)\n",
    "        subscriberCount = get_channel_statistics(channelId)\n",
    "\n",
    "        root = os.getcwd()\n",
    "        file_path = os.path.join(root.split('notebooks')[0], 'data', 'transcripts')\n",
    "\n",
    "        file = open(f'{file_path}/transcript_{video_id}.txt',\"r\")\n",
    "        file_c = open(f'{file_path}/transcript_{video_id}.txt',\"r\")\n",
    "        file_string = file_c.read()\n",
    "        for text in file:\n",
    "            doc_ner = nlp_ner(cleaning_sentence(text)) \n",
    "\n",
    "            for ent in doc_ner.ents:\n",
    "                # print(ent,'->', ent.label_)\n",
    "                \n",
    "                doc = nlp(ent.text)\n",
    "\n",
    "                rating = file_string.count(ent.text) + ((viewCount+likeCount+commentCount+subscriberCount)/(subscriberCount))\n",
    "        \n",
    "                lemma = ' '.join([token.lemma_ for token in doc])\n",
    "                if lemma in ner.keys():\n",
    "                    # ner[ent.label_].append(ent.text)\n",
    "                    ner[lemma]['ENT'].add(ent.label_)\n",
    "                    ner[lemma]['rating'] = (rating+ner[lemma]['rating'])/2\n",
    "                    ner[lemma]['suggested_by'].add(channel_name)\n",
    "                    ner[lemma]['video_id'].add(video_id)\n",
    "                else:\n",
    "                    # ner[ent.label_] = [ent.text]\n",
    "                    ner[lemma] = {\n",
    "                        'word': ent.text,\n",
    "                        'ENT': set([ent.label_]),\n",
    "                        # 'keywords': list,\n",
    "                        'suggested_by': set([channel_name]),\n",
    "                        'rating': rating,\n",
    "                        'video_id':set([video_id])\n",
    "                    }\n",
    "            # print('--x-x-x--x-x-x-x-x-x-x---x-x-x--')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# tokkenization()\n",
    "df = pd.DataFrame(ner)\n",
    "df = df.T\n",
    "df = df.reset_index().rename(columns={'index':'lemma_word'})\n",
    "\n",
    "df['ENT_len'] = df['ENT'].apply(lambda x: len(list(x))) \n",
    "df['ENT_max'] = df['ENT'].apply(lambda x: (list(x))[0])\n",
    "\n",
    "df[df['lemma_word']!=df['word']]\n",
    "\n",
    "df_base_info = df[['lemma_word','word','suggested_by','video_id']]\n",
    "df_entity_info = df[['lemma_word','rating','ENT_max']]\n",
    "\n",
    "df_base_info = df_base_info.explode(['suggested_by','video_id']).drop_duplicates().reset_index()\n",
    "del df_base_info['index']\n",
    "df_base_info['key_word_extracted'] = 0\n",
    "df_base_info.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02002d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma_word         word suggested_by     video_id  key_word_extracted\n",
       "0   sindhi dal   sindhi dal   Singhsman   p0MvovsCxCk                   0\n",
       "1  kudi chhola  kudi chhola   Singhsman   p0MvovsCxCk                   0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_base_info = df[['lemma_word','word','suggested_by','video_id']]\n",
    "# df_entity_info = df[['lemma_word','rating','ENT_max']]\n",
    "\n",
    "# df_base_info = df_base_info.explode(['suggested_by','video_id']).drop_duplicates().reset_index()\n",
    "# del df_base_info['index']\n",
    "# df_base_info['key_word_extracted'] = 0\n",
    "# df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "21e5f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_window(id, text, window_size=50):\n",
    "    root = os.getcwd().split('wanderly.ai')[0]\n",
    "    file_path = os.path.join(root, f'wanderly.ai/data/transcripts/')\n",
    "    file = open(file_path + f'transcript_{id}.txt',\"r\")\n",
    "    \n",
    "    file_string = file.read()\n",
    "    print(file_path + f'transcript_{id}.txt')\n",
    "    # loc = 'city palace'\n",
    "    start = file_string.find(text)\n",
    "    end = file_string.rfind(text)\n",
    "    context_text = (file_string[start-window_size:end+window_size])\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f563531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Oz18u64bM8I.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_p0MvovsCxCk.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_7cPLbiblb84.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_rm_j6O8y148.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Lv0PkSkKeSo.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_5zA6OFpkPe0.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Oz18u64bM8I.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Oz18u64bM8I.txt\n",
      "d:\\randomProjects\\wanderly.ai/data/transcripts/transcript_Oz18u64bM8I.txt\n"
     ]
    }
   ],
   "source": [
    "df_base_info['details'] = df_base_info.apply(lambda x: get_context_window(x['video_id'],x['word'],100),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "796d1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Removes stop words and lemmatizes the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed text with stop words removed and lemmatized.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    filtered_and_lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop and len(token.text)>2]\n",
    "    index = 1\n",
    "    while index<len(filtered_and_lemmatized_tokens):\n",
    "        if filtered_and_lemmatized_tokens[index] == filtered_and_lemmatized_tokens[index-1]:\n",
    "            del filtered_and_lemmatized_tokens[index-1]\n",
    "        else:\n",
    "            index = index+1\n",
    "\n",
    "    return \" \".join(filtered_and_lemmatized_tokens).replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "48e04b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>sindhi dal</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>n have come well in the morning time very much...</td>\n",
       "      <td>come morning time breakfast center breakfast g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>kudi chhola</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>st of our sindhis you have heard that dal dish...</td>\n",
       "      <td>sindhis hear dal dish sindhis people feel good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rice</td>\n",
       "      <td>rice</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>at dal dish in sindhis people feel very good a...</td>\n",
       "      <td>dal dish sindhis people feel good absolutely k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hai pri</td>\n",
       "      <td>hai pri</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>i rice is ok these three or four things are ma...</td>\n",
       "      <td>rice thing maintain morning pricing hai pri si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tamarind sauce</td>\n",
       "      <td>tamarind sauce</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>add ok so the lentils go but you have a little...</td>\n",
       "      <td>add lentil little bit chickpea onion tamarind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>smriti van</td>\n",
       "      <td>smriti van</td>\n",
       "      <td>Sisters vs Globe</td>\n",
       "      <td>5zA6OFpkPe0</td>\n",
       "      <td>0</td>\n",
       "      <td>ht and sound show you more fun\\ncome\\nand it i...</td>\n",
       "      <td>sound fun come unwt 720 morning ring smriti va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>reimber</td>\n",
       "      <td>reimber</td>\n",
       "      <td>Sisters vs Globe</td>\n",
       "      <td>5zA6OFpkPe0</td>\n",
       "      <td>0</td>\n",
       "      <td>walk then any animal also means hands can also...</td>\n",
       "      <td>walk animal mean hand situation leopard reimbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>ram pura</td>\n",
       "      <td>ram pura</td>\n",
       "      <td>Travel Tales</td>\n",
       "      <td>Oz18u64bM8I</td>\n",
       "      <td>0</td>\n",
       "      <td>see very beautiful and very great place and yo...</td>\n",
       "      <td>beautiful great place jaipur second day daya r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>maggi</td>\n",
       "      <td>maggi</td>\n",
       "      <td>Travel Tales</td>\n",
       "      <td>Oz18u64bM8I</td>\n",
       "      <td>0</td>\n",
       "      <td>appreciation\\nthat a appreciation that a do it...</td>\n",
       "      <td>appreciation happen right stay maggi right str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>arun bhai</td>\n",
       "      <td>arun bhai</td>\n",
       "      <td>Travel Tales</td>\n",
       "      <td>Oz18u64bM8I</td>\n",
       "      <td>0</td>\n",
       "      <td>n right now approval currently rapes from here...</td>\n",
       "      <td>right approval currently rape thank video like...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lemma_word            word      suggested_by     video_id  \\\n",
       "0        sindhi dal      sindhi dal        Singhsman   p0MvovsCxCk   \n",
       "1       kudi chhola     kudi chhola        Singhsman   p0MvovsCxCk   \n",
       "2              rice            rice        Singhsman   p0MvovsCxCk   \n",
       "3           hai pri         hai pri        Singhsman   p0MvovsCxCk   \n",
       "4    tamarind sauce  tamarind sauce        Singhsman   p0MvovsCxCk   \n",
       "..              ...             ...               ...          ...   \n",
       "191      smriti van      smriti van  Sisters vs Globe  5zA6OFpkPe0   \n",
       "192         reimber         reimber  Sisters vs Globe  5zA6OFpkPe0   \n",
       "193        ram pura        ram pura      Travel Tales  Oz18u64bM8I   \n",
       "194           maggi           maggi      Travel Tales  Oz18u64bM8I   \n",
       "195       arun bhai       arun bhai      Travel Tales  Oz18u64bM8I   \n",
       "\n",
       "     key_word_extracted                                            details  \\\n",
       "0                     0  n have come well in the morning time very much...   \n",
       "1                     0  st of our sindhis you have heard that dal dish...   \n",
       "2                     0  at dal dish in sindhis people feel very good a...   \n",
       "3                     0  i rice is ok these three or four things are ma...   \n",
       "4                     0  add ok so the lentils go but you have a little...   \n",
       "..                  ...                                                ...   \n",
       "191                   0  ht and sound show you more fun\\ncome\\nand it i...   \n",
       "192                   0  walk then any animal also means hands can also...   \n",
       "193                   0  see very beautiful and very great place and yo...   \n",
       "194                   0  appreciation\\nthat a appreciation that a do it...   \n",
       "195                   0  n right now approval currently rapes from here...   \n",
       "\n",
       "                                            clean_test  \n",
       "0    come morning time breakfast center breakfast g...  \n",
       "1    sindhis hear dal dish sindhis people feel good...  \n",
       "2    dal dish sindhis people feel good absolutely k...  \n",
       "3    rice thing maintain morning pricing hai pri si...  \n",
       "4    add lentil little bit chickpea onion tamarind ...  \n",
       "..                                                 ...  \n",
       "191  sound fun come unwt 720 morning ring smriti va...  \n",
       "192  walk animal mean hand situation leopard reimbe...  \n",
       "193  beautiful great place jaipur second day daya r...  \n",
       "194  appreciation happen right stay maggi right str...  \n",
       "195  right approval currently rape thank video like...  \n",
       "\n",
       "[196 rows x 7 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info['clean_test'] = df_base_info['details'].apply(process_text)\n",
    "df_base_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "12bc82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info = df_base_info.merge(df_entity_info,on=['lemma_word'],how='inner')\n",
    "landmarks = df_base_info[df_base_info['ENT_max'].isin(['LANDMARK','AREA'])].copy().reset_index().drop(columns='index',axis=1)\n",
    "food = df_base_info[df_base_info['ENT_max'].isin(['FOOD','FOOD SHOP'])].copy().reset_index().drop(columns='index',axis=1)\n",
    "# df_base_info = df_base_info[df_base_info['ENT_max'].isin(['LANDMARK','AREA','FOOD','FOOD SHOP'])].copy().reset_index().drop(columns='index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3dee7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info = df_base_info[df_base_info['ENT_max'].isin(['LANDMARK','AREA','FOOD','FOOD SHOP'])].copy().reset_index().drop(columns='index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "90b0c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# cv = CountVectorizer(stop_words='english',ngram_range=(1,2),min_df=0.28)\n",
    "# dim = cv.fit_transform(food['clean_test'])\n",
    "# cv_df = pd.DataFrame(dim.toarray(),columns=cv.get_feature_names_out())\n",
    "# print(cv_df.columns)\n",
    "# cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626cca3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e29b7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['addition', 'addition samosa', 'address', 'address remain', 'ago',\n",
      "       'area', 'arora', 'art', 'bhandar', 'big',\n",
      "       ...\n",
      "       'today', 'today shankar', 'tomato', 'try', 'uncle', 'vegetable',\n",
      "       'wonderful', 'year old', 'year run', 'yes'],\n",
      "      dtype='object', length=136)\n",
      "Index(['1734', '953', '953 window', 'accord', 'accuracy', 'accuracy second',\n",
      "       'address', 'albert', 'amar', 'amar fort',\n",
      "       ...\n",
      "       'water', 'water time', 'way', 'weapon', 'window', 'work', 'world',\n",
      "       'world large', 'write', 'year'],\n",
      "      dtype='object', length=301)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvid_loc = TfidfVectorizer(stop_words='english',ngram_range=(1,2),max_df=0.2,min_df=0.05)\n",
    "X_locations = tvid_loc.fit_transform(landmarks['clean_test'])\n",
    "tvid_loc_df = pd.DataFrame(X_locations.toarray(),columns=tvid_loc.get_feature_names_out())\n",
    "\n",
    "tvid_food = TfidfVectorizer(stop_words='english',ngram_range=(1,2),max_df=0.3,min_df=0.15)\n",
    "X_food = tvid_food.fit_transform(food['clean_test'])\n",
    "tvid_food_df = pd.DataFrame(X_food.toarray(),columns=tvid_food.get_feature_names_out())\n",
    "\n",
    "# tvid_keyWords = TfidfVectorizer(stop_words='english',ngram_range=(1,2),min_df=0.15)\n",
    "# X_keyWords = tvid_keyWords.fit_transform(df_base_info['clean_test'])\n",
    "# tvid_keyWords_df = pd.DataFrame(X_keyWords.toarray(),columns=tvid_keyWords.get_feature_names_out())\n",
    "\n",
    "print(tvid_food_df.columns)\n",
    "print(tvid_loc_df.columns)\n",
    "# print(tvid_keyWords_df.columns)\n",
    "\n",
    "# df_base_info = pd.concat([df_base_info,tvid_keyWords_df],axis=1)\n",
    "# df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ad2a4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = pd.DataFrame(H,index=['Snaks','Evening','Desert','Tikka'])\n",
    "# W = pd.DataFrame(W,columns=['Snaks','Evening','Desert','Tikka'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "187ccfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['hawa', 'hawa mahal', 'floor', 'temple hawa', 'temple floor', 'window', 'temple second', 'ratan', 'prakash temple', 'second vichitra']\n",
      "Topic 2: ['build', 'amer', 'amer fort', 'jaigarh', 'singh', 'jaigarh fort', 'king', 'nahargarh', 'courtyard', 'inr']\n",
      "Topic 3: ['mandir', 'birla', 'park', 'dungri', 'moti dungri', 'moti', 'visit jaipur', 'amar', 'jain', 'similarly']\n",
      "Topic 4: ['good', 'tea', 'year', 'start', 'address', 'father', 'food', 'way', 'morning', 'road']\n",
      "Topic 5: ['jantar', 'mantar', 'jantar mantar', 'instrument', 'museum', 'city palace', 'mean', 'ticket', 'hall', 'hawa mahal']\n",
      "Topic 6: ['lake', 'hai', 'jal mahal', 'jal', 'lake fort', 'box', 'description', 'description box', 'view', 'link description']\n",
      "Topic 7: ['sun', 'little', 'toll', 'bada', 'climb', 'right', 'world large', 'large', 'world', 'lot']\n",
      "Topic 1: ['samosa', 'start', 'sir', 'eat', 'chilli', 'address', 'remain', 'shankar samosa', 'shankar', 'good address']\n",
      "Topic 2: ['evening', 'let', 'time', 'wonderful', 'flavor', 'uncle', 'search', 'brother', 'look', 'taste']\n",
      "Topic 3: ['tikka', 'cheese', 'paneer', 'paneer tikka', 'vegetable', 'cheese tikka', 'tomato', 'inside', 'famous', 'mix']\n",
      "Topic 4: ['rabri', 'mahavir', 'potato onion', 'potato', 'share', 'shop', 'store mishra', 'stand mahavir', 'jaipur share', 'mahavir come']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin Arora\\AppData\\Local\\Temp\\ipykernel_9820\\447552213.py:27: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_base_info = pd.concat([landmarks,food]).fillna(0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>rating</th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>Forts</th>\n",
       "      <th>Generic</th>\n",
       "      <th>Temple</th>\n",
       "      <th>Temples &amp; Architectur</th>\n",
       "      <th>Scientific Monuments</th>\n",
       "      <th>Lakeside Forts</th>\n",
       "      <th>Views</th>\n",
       "      <th>Snaks</th>\n",
       "      <th>Evening</th>\n",
       "      <th>Desert</th>\n",
       "      <th>Tikka</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai pri</td>\n",
       "      <td>hai pri</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>i rice is ok these three or four things are ma...</td>\n",
       "      <td>rice thing maintain morning pricing hai pri si...</td>\n",
       "      <td>19.909299</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184352</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.162517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allredi trai</td>\n",
       "      <td>allredi trai</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>e lentil and of tapre mirchi will keep my effo...</td>\n",
       "      <td>lentil tapre mirchi effort basically let thing...</td>\n",
       "      <td>19.909299</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079970</td>\n",
       "      <td>0.00858</td>\n",
       "      <td>0.080938</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lemma_word          word suggested_by     video_id  key_word_extracted  \\\n",
       "0       hai pri       hai pri   Singhsman   p0MvovsCxCk                   0   \n",
       "1  allredi trai  allredi trai   Singhsman   p0MvovsCxCk                   0   \n",
       "\n",
       "                                             details  \\\n",
       "0  i rice is ok these three or four things are ma...   \n",
       "1  e lentil and of tapre mirchi will keep my effo...   \n",
       "\n",
       "                                          clean_test     rating   ENT_max  \\\n",
       "0  rice thing maintain morning pricing hai pri si...  19.909299  LANDMARK   \n",
       "1  lentil tapre mirchi effort basically let thing...  19.909299  LANDMARK   \n",
       "\n",
       "   Forts  Generic  Temple  Temples & Architectur  Scientific Monuments  \\\n",
       "0    0.0      0.0     0.0               0.184352               0.00000   \n",
       "1    0.0      0.0     0.0               0.079970               0.00858   \n",
       "\n",
       "   Lakeside Forts     Views  Snaks  Evening  Desert  Tikka  \n",
       "0        0.162517  0.000000    0.0      0.0     0.0    0.0  \n",
       "1        0.080938  0.030002    0.0      0.0     0.0    0.0  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_model_loc = NMF(n_components=7)\n",
    "W_locations = nmf_model_loc.fit_transform(X_locations)\n",
    "H_locations = nmf_model_loc.components_\n",
    "\n",
    "nmf_model_food = NMF(n_components=4)\n",
    "W_food = nmf_model_food.fit_transform(X_food)\n",
    "H_food = nmf_model_food.components_\n",
    "\n",
    "def display_topic(H,tvid):\n",
    "    for topic_num, topic_array in enumerate(H):\n",
    "        top_features = topic_array.argsort()[::-1][:10]\n",
    "        top_words = [tvid.get_feature_names_out()[i] for i in top_features]\n",
    "        print(f\"Topic {topic_num +1}: {top_words}\")\n",
    "\n",
    "display_topic(H_locations,tvid_loc)\n",
    "display_topic(H_food,tvid_food)\n",
    "# H = pd.DataFrame(H,index=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "W_locations = pd.DataFrame(W_locations,columns=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "\n",
    "# H = pd.DataFrame(H,index=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "W_food = pd.DataFrame(W_food,columns=['Snaks','Evening','Desert','Tikka'])\n",
    "\n",
    "food = pd.concat([food,W_food],axis=1)\n",
    "landmarks = pd.concat([landmarks,W_locations],axis=1)\n",
    "\n",
    "df_base_info = pd.concat([landmarks,food]).fillna(0)\n",
    "df_base_info.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "57837c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['build', 'city', 'come', 'day', 'food', 'fort', 'good', 'inside',\n",
      "       'jaipur', 'king', 'know', 'like', 'look', 'lot', 'mahal', 'palace',\n",
      "       'people', 'place', 'singh', 'temple', 'thing', 'time', 'timing', 'view',\n",
      "       'visit', 'way', 'world', 'year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tvid_keyWords = TfidfVectorizer(stop_words='english',ngram_range=(1,2),min_df=0.15)\n",
    "X_keyWords = tvid_keyWords.fit_transform(df_base_info['clean_test'])\n",
    "tvid_keyWords_df = pd.DataFrame(X_keyWords.toarray(),columns=tvid_keyWords.get_feature_names_out())\n",
    "\n",
    "print(tvid_keyWords_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "f99fa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_info = df_base_info.reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1472c28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>rating</th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>Forts</th>\n",
       "      <th>...</th>\n",
       "      <th>singh</th>\n",
       "      <th>temple</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>timing</th>\n",
       "      <th>view</th>\n",
       "      <th>visit</th>\n",
       "      <th>way</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai pri</td>\n",
       "      <td>hai pri</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>i rice is ok these three or four things are ma...</td>\n",
       "      <td>rice thing maintain morning pricing hai pri si...</td>\n",
       "      <td>19.909299</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allredi trai</td>\n",
       "      <td>allredi trai</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>e lentil and of tapre mirchi will keep my effo...</td>\n",
       "      <td>lentil tapre mirchi effort basically let thing...</td>\n",
       "      <td>19.909299</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lemma_word          word suggested_by     video_id  key_word_extracted  \\\n",
       "0       hai pri       hai pri   Singhsman   p0MvovsCxCk                   0   \n",
       "1  allredi trai  allredi trai   Singhsman   p0MvovsCxCk                   0   \n",
       "\n",
       "                                             details  \\\n",
       "0  i rice is ok these three or four things are ma...   \n",
       "1  e lentil and of tapre mirchi will keep my effo...   \n",
       "\n",
       "                                          clean_test     rating   ENT_max  \\\n",
       "0  rice thing maintain morning pricing hai pri si...  19.909299  LANDMARK   \n",
       "1  lentil tapre mirchi effort basically let thing...  19.909299  LANDMARK   \n",
       "\n",
       "   Forts  ...  singh  temple     thing  time  timing  view  visit  way  world  \\\n",
       "0    0.0  ...    0.0     0.0  0.907248   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "1    0.0  ...    0.0     0.0  0.667711   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "\n",
       "   year  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "\n",
       "[2 rows x 48 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info = pd.concat([df_base_info,tvid_keyWords_df],axis=1)\n",
    "df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmarks = pd.concat([landmarks,W],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# food = pd.concat([food,W],axis=1)\n",
    "# landmarks = pd.concat([landmarks,W],axis=1)\n",
    "# df_base_info = pd.concat([landmarks,food]).fillna(0)\n",
    "# df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_info = pd.concat([landmarks,food]).fillna(0)\n",
    "# df_base_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_rateing(max_r,min_r,current_r):\n",
    "#     return (current_r - min_r)/(max_r + min_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f1f6415a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>min_category_rating</th>\n",
       "      <th>max_category_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AREA</td>\n",
       "      <td>2.372537</td>\n",
       "      <td>43.909299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOOD</td>\n",
       "      <td>2.660253</td>\n",
       "      <td>35.640255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOOD SHOP</td>\n",
       "      <td>2.372537</td>\n",
       "      <td>35.640255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>2.372537</td>\n",
       "      <td>52.639913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ENT_max  min_category_rating  max_category_rating\n",
       "0       AREA             2.372537            43.909299\n",
       "1       FOOD             2.660253            35.640255\n",
       "2  FOOD SHOP             2.372537            35.640255\n",
       "3   LANDMARK             2.372537            52.639913"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_category_rateing = df_base_info.groupby(by='ENT_max')['rating'].agg(['min','max']).reset_index().rename(columns={'min':'min_category_rating','max':'max_category_rating'})\n",
    "min_max_category_rateing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "477ee73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>rating</th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>Forts</th>\n",
       "      <th>...</th>\n",
       "      <th>temple</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>timing</th>\n",
       "      <th>view</th>\n",
       "      <th>visit</th>\n",
       "      <th>way</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>normalize_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hai pri</td>\n",
       "      <td>hai pri</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>i rice is ok these three or four things are ma...</td>\n",
       "      <td>rice thing maintain morning pricing hai pri si...</td>\n",
       "      <td>19.909299</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allredi trai</td>\n",
       "      <td>allredi trai</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>e lentil and of tapre mirchi will keep my effo...</td>\n",
       "      <td>lentil tapre mirchi effort basically let thing...</td>\n",
       "      <td>19.909299</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allredi</td>\n",
       "      <td>allredi</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>e lentil and of tapre mirchi will keep my effo...</td>\n",
       "      <td>lentil tapre mirchi effort basically let thing...</td>\n",
       "      <td>20.909299</td>\n",
       "      <td>AREA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.576889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nana ji</td>\n",
       "      <td>nana ji</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>name sir gulshan bhatia sir our father started...</td>\n",
       "      <td>sir gulshan bhatia sir father start paneer goo...</td>\n",
       "      <td>19.909299</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>vaccines</td>\n",
       "      <td>Singhsman</td>\n",
       "      <td>p0MvovsCxCk</td>\n",
       "      <td>0</td>\n",
       "      <td>something when the pure cheese was with you a...</td>\n",
       "      <td>pure cheese blessing cheese introduction vacci...</td>\n",
       "      <td>19.909299</td>\n",
       "      <td>AREA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.378912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>bharat</td>\n",
       "      <td>bharat</td>\n",
       "      <td>Aniruddha Patil</td>\n",
       "      <td>Lv0PkSkKeSo</td>\n",
       "      <td>0</td>\n",
       "      <td>go and go with that the being sed i will c yo...</td>\n",
       "      <td>se till den tech care bye jai hind jai bharat</td>\n",
       "      <td>2.372537</td>\n",
       "      <td>FOOD SHOP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>aamir food</td>\n",
       "      <td>aamir food</td>\n",
       "      <td>Sisters vs Globe</td>\n",
       "      <td>5zA6OFpkPe0</td>\n",
       "      <td>0</td>\n",
       "      <td>comes to jaipur take the america will also sho...</td>\n",
       "      <td>come jaipur america thank brother appreciation...</td>\n",
       "      <td>2.660253</td>\n",
       "      <td>FOOD SHOP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>reimber</td>\n",
       "      <td>reimber</td>\n",
       "      <td>Sisters vs Globe</td>\n",
       "      <td>5zA6OFpkPe0</td>\n",
       "      <td>0</td>\n",
       "      <td>walk then any animal also means hands can also...</td>\n",
       "      <td>walk animal mean hand situation leopard reimbe...</td>\n",
       "      <td>2.660253</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>maggi</td>\n",
       "      <td>maggi</td>\n",
       "      <td>Travel Tales</td>\n",
       "      <td>Oz18u64bM8I</td>\n",
       "      <td>0</td>\n",
       "      <td>appreciation\\nthat a appreciation that a do it...</td>\n",
       "      <td>appreciation happen right stay maggi right str...</td>\n",
       "      <td>3.013400</td>\n",
       "      <td>FOOD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>arun bhai</td>\n",
       "      <td>arun bhai</td>\n",
       "      <td>Travel Tales</td>\n",
       "      <td>Oz18u64bM8I</td>\n",
       "      <td>0</td>\n",
       "      <td>n right now approval currently rapes from here...</td>\n",
       "      <td>right approval currently rape thank video like...</td>\n",
       "      <td>3.013400</td>\n",
       "      <td>FOOD SHOP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lemma_word          word      suggested_by     video_id  \\\n",
       "0         hai pri       hai pri        Singhsman   p0MvovsCxCk   \n",
       "1    allredi trai  allredi trai        Singhsman   p0MvovsCxCk   \n",
       "2         allredi       allredi        Singhsman   p0MvovsCxCk   \n",
       "3         nana ji       nana ji        Singhsman   p0MvovsCxCk   \n",
       "4         vaccine      vaccines        Singhsman   p0MvovsCxCk   \n",
       "..            ...           ...               ...          ...   \n",
       "174        bharat        bharat   Aniruddha Patil  Lv0PkSkKeSo   \n",
       "175    aamir food    aamir food  Sisters vs Globe  5zA6OFpkPe0   \n",
       "176       reimber       reimber  Sisters vs Globe  5zA6OFpkPe0   \n",
       "177         maggi         maggi      Travel Tales  Oz18u64bM8I   \n",
       "178     arun bhai     arun bhai      Travel Tales  Oz18u64bM8I   \n",
       "\n",
       "     key_word_extracted                                            details  \\\n",
       "0                     0  i rice is ok these three or four things are ma...   \n",
       "1                     0  e lentil and of tapre mirchi will keep my effo...   \n",
       "2                     0  e lentil and of tapre mirchi will keep my effo...   \n",
       "3                     0  name sir gulshan bhatia sir our father started...   \n",
       "4                     0   something when the pure cheese was with you a...   \n",
       "..                  ...                                                ...   \n",
       "174                   0   go and go with that the being sed i will c yo...   \n",
       "175                   0  comes to jaipur take the america will also sho...   \n",
       "176                   0  walk then any animal also means hands can also...   \n",
       "177                   0  appreciation\\nthat a appreciation that a do it...   \n",
       "178                   0  n right now approval currently rapes from here...   \n",
       "\n",
       "                                            clean_test     rating    ENT_max  \\\n",
       "0    rice thing maintain morning pricing hai pri si...  19.909299   LANDMARK   \n",
       "1    lentil tapre mirchi effort basically let thing...  19.909299   LANDMARK   \n",
       "2    lentil tapre mirchi effort basically let thing...  20.909299       AREA   \n",
       "3    sir gulshan bhatia sir father start paneer goo...  19.909299   LANDMARK   \n",
       "4    pure cheese blessing cheese introduction vacci...  19.909299       AREA   \n",
       "..                                                 ...        ...        ...   \n",
       "174      se till den tech care bye jai hind jai bharat   2.372537  FOOD SHOP   \n",
       "175  come jaipur america thank brother appreciation...   2.660253  FOOD SHOP   \n",
       "176  walk animal mean hand situation leopard reimbe...   2.660253       FOOD   \n",
       "177  appreciation happen right stay maggi right str...   3.013400       FOOD   \n",
       "178  right approval currently rape thank video like...   3.013400  FOOD SHOP   \n",
       "\n",
       "     Forts  ...  temple     thing  time  timing  view  visit  way  world  \\\n",
       "0      0.0  ...     0.0  0.907248   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "1      0.0  ...     0.0  0.667711   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "2      0.0  ...     0.0  0.576889   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "3      0.0  ...     0.0  0.000000   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "4      0.0  ...     0.0  0.000000   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "..     ...  ...     ...       ...   ...     ...   ...    ...  ...    ...   \n",
       "174    0.0  ...     0.0  0.000000   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "175    0.0  ...     0.0  0.000000   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "176    0.0  ...     0.0  0.000000   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "177    0.0  ...     0.0  0.000000   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "178    0.0  ...     0.0  0.000000   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "\n",
       "     year  normalize_rating  \n",
       "0     0.0          0.318778  \n",
       "1     0.0          0.318778  \n",
       "2     0.0          0.400519  \n",
       "3     0.0          0.318778  \n",
       "4     1.0          0.378912  \n",
       "..    ...               ...  \n",
       "174   0.0          0.000000  \n",
       "175   0.0          0.007569  \n",
       "176   0.0          0.000000  \n",
       "177   0.0          0.009220  \n",
       "178   0.0          0.016859  \n",
       "\n",
       "[179 rows x 49 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info = df_base_info.merge(min_max_category_rateing,on='ENT_max')\n",
    "df_base_info['normalize_rating'] = (df_base_info['rating'] - df_base_info['min_category_rating'])/(df_base_info['max_category_rating'] + df_base_info['min_category_rating'])\n",
    "df_base_info.drop(['min_category_rating','max_category_rating'],axis=1,inplace=True)\n",
    "df_base_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25606329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4bdddf96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lemma_word', 'word', 'suggested_by', 'video_id', 'key_word_extracted',\n",
       "       'details', 'clean_test', 'rating', 'ENT_max', 'Forts', 'Generic',\n",
       "       'Temple', 'Temples & Architectur', 'Scientific Monuments',\n",
       "       'Lakeside Forts', 'Views', 'Snaks', 'Evening', 'Desert', 'Tikka',\n",
       "       'build', 'city', 'come', 'day', 'food', 'fort', 'good', 'inside',\n",
       "       'jaipur', 'king', 'know', 'like', 'look', 'lot', 'mahal', 'palace',\n",
       "       'people', 'place', 'singh', 'temple', 'thing', 'time', 'timing', 'view',\n",
       "       'visit', 'way', 'world', 'year', 'normalize_rating'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "551e388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_col = ['Forts', 'Generic',\n",
    "       'Temple', 'Temples & Architectur', 'Scientific Monuments',\n",
    "       'Lakeside Forts', 'Views', 'Snaks', 'Evening', 'Desert', 'Tikka',\n",
    "       'normalize_rating', 'build', 'city', 'come', 'day', 'food',\n",
    "       'fort', 'good', 'inside', 'jaipur', 'king', 'know', 'like', 'look',\n",
    "       'lot', 'mahal', 'palace', 'people', 'place', 'singh', 'temple', 'thing',\n",
    "       'time', 'timing', 'view', 'visit', 'way', 'world', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8418b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_base_info[req_col]\n",
    "Y = df_base_info[['lemma_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587bdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhat excites you most about traveling?\\nWhich of these travel quotes resonates with you the most?\\nIf you had a 3-day break, what would you do?\\nWhat type of destinations do you find most appealing?\\nWhat kind of experiences do you look for while traveling?\\nWhat do you often post or talk about after your trips?\\nWhich of these describe you best when traveling?\\nWhat's the first thing you usually do after reaching a new place?\\nDescribe your dream travel day in 2â€“3 sentences.\\n\""
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "What excites you most about traveling?\n",
    "Which of these travel quotes resonates with you the most?\n",
    "If you had a 3-day break, what would you do?\n",
    "What type of destinations do you find most appealing?\n",
    "What kind of experiences do you look for while traveling?\n",
    "What do you often post or talk about after your trips?\n",
    "Which of these describe you best when traveling?\n",
    "What's the first thing you usually do after reaching a new place?\n",
    "Describe your dream travel day in 2â€“3 sentences.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0cb8d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = ['I like to trave to find me, enjoy, capture the movement',\n",
    "'Like to explore the culture, heritage and local food',\n",
    "'relax chill with friends',\n",
    "'peacefull place calm and relaxing',\n",
    "'chill good local food instagram worthy locations, vloging',\n",
    "'Explorer â€“ I love wandering and getting lost',\n",
    "'Explore the surroundings on foot',\n",
    "'Dream travel worule be lost in the movement, relax, local food, chill vibes, haritage, mountain person but \\\n",
    "also like beaches and water side resorts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "5107a137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to trave to find me, enjoy, capture the movement Like to explore the culture, heritage and local food relax chill with friends peacefull place calm and relaxing chill good local food instagram worthy locations, vloging Explorer â€“ I love wandering and getting lost Explore the surroundings on foot Dream travel worule be lost in the movement, relax, local food, chill vibes, haritage, mountain person but also like beaches and water side resorts\n"
     ]
    }
   ],
   "source": [
    "user_input = ' '.join(user_input)\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "fd6507a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jatin</td>\n",
       "      <td>I like to trave to find me, enjoy, capture the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name                                               desc\n",
       "0  Jatin  I like to trave to find me, enjoy, capture the..."
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = pd.DataFrame({'name':['Jatin'],'desc':[user_input]})\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2ee9867c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "      <th>clean_dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jatin</td>\n",
       "      <td>I like to trave to find me, enjoy, capture the...</td>\n",
       "      <td>like trave find enjoy capture movement like ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name                                               desc  \\\n",
       "0  Jatin  I like to trave to find me, enjoy, capture the...   \n",
       "\n",
       "                                           clean_dec  \n",
       "0  like trave find enjoy capture movement like ex...  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user['clean_dec'] = user['desc'].apply(lambda x: process_text(cleaning_sentence(x)))\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2f9f767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_locations = tvid_loc.transform(user['clean_dec'])\n",
    "tvid_loc_df = pd.DataFrame(X_locations.toarray(),columns=tvid_loc.get_feature_names_out())\n",
    "\n",
    "X_food = tvid_food.transform(user['clean_dec'])\n",
    "tvid_food_df = pd.DataFrame(X_food.toarray(),columns=tvid_food.get_feature_names_out())\n",
    "\n",
    "X_keyWords = tvid_keyWords.transform(user['clean_dec'])\n",
    "tvid_keyWords_df = pd.DataFrame(X_keyWords.toarray(),columns=tvid_keyWords.get_feature_names_out())\n",
    "# tvid_keyWords_df = pd.DataFrame(X_keyWords.toarray(),columns=tvid_keyWords.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "257e2515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>build</th>\n",
       "      <th>city</th>\n",
       "      <th>come</th>\n",
       "      <th>day</th>\n",
       "      <th>food</th>\n",
       "      <th>fort</th>\n",
       "      <th>good</th>\n",
       "      <th>inside</th>\n",
       "      <th>jaipur</th>\n",
       "      <th>king</th>\n",
       "      <th>...</th>\n",
       "      <th>singh</th>\n",
       "      <th>temple</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>timing</th>\n",
       "      <th>view</th>\n",
       "      <th>visit</th>\n",
       "      <th>way</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   build  city  come  day      food  fort      good  inside  jaipur  king  \\\n",
       "0    0.0   0.0   0.0  0.0  0.699599   0.0  0.191613     0.0     0.0   0.0   \n",
       "\n",
       "   ...  singh  temple  thing  time  timing  view  visit  way  world  year  \n",
       "0  ...    0.0     0.0    0.0   0.0     0.0   0.0    0.0  0.0    0.0   0.0  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvid_keyWords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "f36af3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_locations = nmf_model_loc.transform(X_locations)\n",
    "H_locations = nmf_model_loc.components_\n",
    "\n",
    "W_food = nmf_model_food.transform(X_food)\n",
    "H_food = nmf_model_food.components_\n",
    "\n",
    "W_locations = pd.DataFrame(W_locations,columns=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "\n",
    "# H = pd.DataFrame(H,index=['Forts','Generic','Temple','Temples & Architectur','Scientific Monuments','Lakeside Forts','Views'])\n",
    "W_food = pd.DataFrame(W_food,columns=['Snaks','Evening','Desert','Tikka'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "57c7d5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(W_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c62ce00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(W_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "5609d571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>desc</th>\n",
       "      <th>clean_dec</th>\n",
       "      <th>build</th>\n",
       "      <th>city</th>\n",
       "      <th>come</th>\n",
       "      <th>day</th>\n",
       "      <th>food</th>\n",
       "      <th>fort</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>Generic</th>\n",
       "      <th>Temple</th>\n",
       "      <th>Temples &amp; Architectur</th>\n",
       "      <th>Scientific Monuments</th>\n",
       "      <th>Lakeside Forts</th>\n",
       "      <th>Views</th>\n",
       "      <th>Snaks</th>\n",
       "      <th>Evening</th>\n",
       "      <th>Desert</th>\n",
       "      <th>Tikka</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jatin</td>\n",
       "      <td>I like to trave to find me, enjoy, capture the...</td>\n",
       "      <td>like trave find enjoy capture movement like ex...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108836</td>\n",
       "      <td>0.040124</td>\n",
       "      <td>0.055411</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.051871</td>\n",
       "      <td>0.144062</td>\n",
       "      <td>0.00303</td>\n",
       "      <td>0.011624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name                                               desc  \\\n",
       "0  Jatin  I like to trave to find me, enjoy, capture the...   \n",
       "\n",
       "                                           clean_dec  build  city  come  day  \\\n",
       "0  like trave find enjoy capture movement like ex...    0.0   0.0   0.0  0.0   \n",
       "\n",
       "       food  fort      good  ...   Generic  Temple  Temples & Architectur  \\\n",
       "0  0.699599   0.0  0.191613  ...  0.025911     0.0               0.108836   \n",
       "\n",
       "   Scientific Monuments  Lakeside Forts     Views     Snaks   Evening  \\\n",
       "0              0.040124        0.055411  0.003221  0.051871  0.144062   \n",
       "\n",
       "    Desert     Tikka  \n",
       "0  0.00303  0.011624  \n",
       "\n",
       "[1 rows x 42 columns]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_transform = pd.concat([user,tvid_keyWords_df,W_locations,W_food],axis=1)\n",
    "user_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e0fa3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_similar_objects(check,X,Y,top_n=5):\n",
    "    common = [i for i in X.columns if i in check.columns]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X[common])\n",
    "    user_scaled = scaler.transform(check[common])\n",
    "    \n",
    "    # Compute cosine similarity between user vector and all item vectors\n",
    "    similarity_scores = cosine_similarity(user_scaled, X[common])[0]\n",
    "\n",
    "    # Add scores to DataFrame\n",
    "    Y['similarity'] = similarity_scores\n",
    "\n",
    "    # Sort top recommendations\n",
    "    top_recommendations = Y.sort_values(by='similarity', ascending=False).head(10)\n",
    "    return top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "8e9bb7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin Arora\\AppData\\Local\\Temp\\ipykernel_9820\\2801194235.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Y['similarity'] = similarity_scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>subscribe</td>\n",
       "      <td>0.633513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sodala</td>\n",
       "      <td>0.598635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>arun bhai</td>\n",
       "      <td>0.591013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>chowki dhani</td>\n",
       "      <td>0.558696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>dhaba</td>\n",
       "      <td>0.492874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>sambar lake</td>\n",
       "      <td>0.462036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gulab ji</td>\n",
       "      <td>0.447595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>chaat</td>\n",
       "      <td>0.392232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>aamir food</td>\n",
       "      <td>0.376524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>raja park</td>\n",
       "      <td>0.360683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lemma_word  similarity\n",
       "128     subscribe    0.633513\n",
       "7          sodala    0.598635\n",
       "178     arun bhai    0.591013\n",
       "40   chowki dhani    0.558696\n",
       "166         dhaba    0.492874\n",
       "127   sambar lake    0.462036\n",
       "8        gulab ji    0.447595\n",
       "163         chaat    0.392232\n",
       "175    aamir food    0.376524\n",
       "5       raja park    0.360683"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_objects(user_transform,X,Y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "559164e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>word</th>\n",
       "      <th>suggested_by</th>\n",
       "      <th>video_id</th>\n",
       "      <th>key_word_extracted</th>\n",
       "      <th>details</th>\n",
       "      <th>clean_test</th>\n",
       "      <th>rating</th>\n",
       "      <th>ENT_max</th>\n",
       "      <th>Forts</th>\n",
       "      <th>...</th>\n",
       "      <th>temple</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>timing</th>\n",
       "      <th>view</th>\n",
       "      <th>visit</th>\n",
       "      <th>way</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>normalize_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>sambar lake</td>\n",
       "      <td>sambar lake</td>\n",
       "      <td>Aniruddha Patil</td>\n",
       "      <td>Lv0PkSkKeSo</td>\n",
       "      <td>0</td>\n",
       "      <td>m jaipur a distance of 75 km but indias bigges...</td>\n",
       "      <td>jaipur distance india big salt settle water la...</td>\n",
       "      <td>6.372537</td>\n",
       "      <td>LANDMARK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.072711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma_word         word     suggested_by     video_id  \\\n",
       "127  sambar lake  sambar lake  Aniruddha Patil  Lv0PkSkKeSo   \n",
       "\n",
       "     key_word_extracted                                            details  \\\n",
       "127                   0  m jaipur a distance of 75 km but indias bigges...   \n",
       "\n",
       "                                            clean_test    rating   ENT_max  \\\n",
       "127  jaipur distance india big salt settle water la...  6.372537  LANDMARK   \n",
       "\n",
       "     Forts  ...  temple  thing  time  timing  view  visit  way  world  \\\n",
       "127    0.0  ...     0.0    0.0   0.0     0.0   0.0    0.0  0.0    0.0   \n",
       "\n",
       "         year  normalize_rating  \n",
       "127  0.456381          0.072711  \n",
       "\n",
       "[1 rows x 49 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base_info[df_base_info['lemma_word']=='sambar lake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "27e20264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin Arora\\AppData\\Local\\Temp\\ipykernel_9820\\2013376519.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Y['similarity'] = similarity_scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>chowki dhani</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sodala</td>\n",
       "      <td>0.825258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>dhaba</td>\n",
       "      <td>0.771376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>nahargarh</td>\n",
       "      <td>0.639082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gulab ji</td>\n",
       "      <td>0.581485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>shankar samosa</td>\n",
       "      <td>0.564242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>shimla tutter</td>\n",
       "      <td>0.540364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>maggi</td>\n",
       "      <td>0.537956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hawa mahal</td>\n",
       "      <td>0.537956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>tamarind sauce</td>\n",
       "      <td>0.523520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lemma_word  similarity\n",
       "40     chowki dhani    1.000000\n",
       "7            sodala    0.825258\n",
       "166           dhaba    0.771376\n",
       "56        nahargarh    0.639082\n",
       "8          gulab ji    0.581485\n",
       "153  shankar samosa    0.564242\n",
       "6     shimla tutter    0.540364\n",
       "177           maggi    0.537956\n",
       "16       hawa mahal    0.537956\n",
       "141  tamarind sauce    0.523520"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_objects(df_base_info[df_base_info['lemma_word']=='chowki dhani'],X,Y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afcb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_word</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pune</td>\n",
       "      <td>0.656606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>zenana deodi</td>\n",
       "      <td>0.613180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nana ji</td>\n",
       "      <td>0.607636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>mubarak mahal</td>\n",
       "      <td>0.546297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>chaat</td>\n",
       "      <td>0.546149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>palace of reception</td>\n",
       "      <td>0.519179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>arun bhai</td>\n",
       "      <td>0.513964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hawa mahal</td>\n",
       "      <td>0.482137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>raman dosa</td>\n",
       "      <td>0.380070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>amer fort</td>\n",
       "      <td>0.370288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lemma_word  similarity\n",
       "27                  pune    0.656606\n",
       "85          zenana deodi    0.613180\n",
       "9                nana ji    0.607636\n",
       "49         mubarak mahal    0.546297\n",
       "180                chaat    0.546149\n",
       "50   palace of reception    0.519179\n",
       "195            arun bhai    0.513964\n",
       "28            hawa mahal    0.482137\n",
       "178           raman dosa    0.380070\n",
       "57             amer fort    0.370288"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_recommendations = Y.sort_values(by='similarity', ascending=False).head(10)\n",
    "top_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import required modules after environment reset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the main function to recommend keywords\n",
    "def recommend_keywords_for_user(\n",
    "    user_input,\n",
    "    df_base_info,\n",
    "    tfidf_models,\n",
    "    nmf_models,\n",
    "    keyword_vectorizer,\n",
    "    process_text_func,\n",
    "    req_col,\n",
    "    topic_labels,\n",
    "    normalize=True,\n",
    "    top_n=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Full keyword recommendation pipeline from raw user input.\n",
    "\n",
    "    Args:\n",
    "        user_input (str): Raw user description or answers.\n",
    "        df_base_info (pd.DataFrame): DataFrame containing features and metadata.\n",
    "        tfidf_models (dict): Dictionary with 'location' and 'food' TfidfVectorizers.\n",
    "        nmf_models (dict): Dictionary with 'location' and 'food' NMF models.\n",
    "        keyword_vectorizer: TF-IDF vectorizer used for general keywords.\n",
    "        process_text_func (callable): Text cleaning and lemmatization function.\n",
    "        req_col (list): Final column names used in cosine similarity.\n",
    "        topic_labels (dict): {'location': [...], 'food': [...]}\n",
    "        normalize (bool): Whether to scale all features.\n",
    "        top_n (int): Number of recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Top N recommended keywords/entities.\n",
    "    \"\"\"\n",
    "    # 1. Clean and prepare user input\n",
    "    user_clean = process_text_func(user_input)\n",
    "\n",
    "    # 2. Transform into TF-IDF vectors\n",
    "    X_loc = tfidf_models['location'].transform([user_clean])\n",
    "    X_food = tfidf_models['food'].transform([user_clean])\n",
    "    X_key = keyword_vectorizer.transform([user_clean])\n",
    "\n",
    "    # 3. Apply NMF topic models\n",
    "    W_loc = nmf_models['location'].transform(X_loc)\n",
    "    W_food = nmf_models['food'].transform(X_food)\n",
    "\n",
    "    # 4. Build user feature vector\n",
    "    user_vector = np.concatenate([\n",
    "        W_loc[0],\n",
    "        W_food[0],\n",
    "        [0.5],  # normalize_rating neutral\n",
    "        X_key.toarray()[0][:len(req_col) - (len(W_loc[0]) + len(W_food[0]) + 1)]\n",
    "    ])\n",
    "    user_vector = pd.DataFrame([user_vector], columns=req_col[:len(user_vector)])\n",
    "\n",
    "    # 5. Feature scaling (optional)\n",
    "    if normalize:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(df_base_info[req_col])\n",
    "        user_scaled = scaler.transform(user_vector)\n",
    "    else:\n",
    "        X_scaled = df_base_info[req_col].values\n",
    "        user_scaled = user_vector.values\n",
    "\n",
    "    # 6. Cosine similarity\n",
    "    similarity_scores = cosine_similarity(user_scaled, X_scaled)[0]\n",
    "    df_result = df_base_info.copy()\n",
    "    df_result['score'] = similarity_scores\n",
    "\n",
    "    # 7. Return top N\n",
    "    return df_result.sort_values(by='score', ascending=False).head(top_n)[\n",
    "        ['lemma_word', 'ENT_max', 'video_id', 'suggested_by', 'score']\n",
    "    ]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
